# Day 1: Installation & First DAG

**Time:** 1-2 hours  
**Goal:** Install Airflow locally and run your first DAG.

---

## ðŸ“š Airflow Architecture (Quick Recap)

```
Components you'll install:
â”œâ”€â”€ Web Server   (UI at localhost:8080)
â”œâ”€â”€ Scheduler    (Triggers tasks)
â”œâ”€â”€ Database     (Metadata - uses SQLite by default)
â””â”€â”€ Executor     (SequentialExecutor for local dev)
```

---

## ðŸ›  Installation

### Option 1: pip (Recommended for Learning)

**Step 1: Create virtual environment**
```bash
python -m venv airflow_venv
source airflow_venv/bin/activate  # On Windows: airflow_venv\Scripts\activate
```

**Step 2: Set Airflow home**
```bash
export AIRFLOW_HOME=~/airflow  # On Windows: set AIRFLOW_HOME=%USERPROFILE%\airflow
```

**Step 3: Install Airflow**
```bash
AIRFLOW_VERSION=2.8.0
PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"

pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"
```

**Step 4: Initialize database**
```bash
airflow db init
```

**Step 5: Create admin user**
```bash
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin
```

**Step 6: Start services**
```bash
# Terminal 1: Webserver
airflow webserver --port 8080

# Terminal 2: Scheduler
airflow scheduler
```

**Step 7: Access UI**
- Navigate to `http://localhost:8080`
- Login: `admin` / `admin`

---

### Option 2: Docker (Alternative)

```bash
# Download docker-compose.yaml
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.8.0/docker-compose.yaml'

# Initialize
docker-compose up airflow-init

# Start all services
docker-compose up
```

---

## ðŸŽ¯ Your First DAG

### Step 1: Create DAG file

**Location:** `$AIRFLOW_HOME/dags/hello_world.py`

```python
from datetime import datetime
from airflow import DAG
from airflow.operators.bash import BashOperator

# Define default arguments
default_args = {
    'owner': 'data_engineer',
    'retries': 1,
    'start_date': datetime(2025, 12, 1),
}

# Create the DAG
with DAG(
    dag_id='hello_world',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
    tags=['tutorial', 'beginner'],
) as dag:
    
    # Task 1: Print hello
    hello = BashOperator(
        task_id='say_hello',
        bash_command='echo "Hello, Airflow!"',
    )
    
    # Task 2: Print date
    print_date = BashOperator(
        task_id='print_date',
        bash_command='date',
    )
    
    # Task 3: Sleep
    wait = BashOperator(
        task_id='wait_5_seconds',
        bash_command='sleep 5',
    )
    
    # Task 4: Print goodbye
    goodbye = BashOperator(
        task_id='say_goodbye',
        bash_command='echo "Goodbye!"',
    )
    
    # Define dependencies
    hello >> print_date >> wait >> goodbye
```

### Step 2: Verify DAG appears

```bash
# List all DAGs
airflow dags list

# Check for errors
airflow dags list-import-errors
```

### Step 3: Trigger the DAG

**Via UI:**
1. Go to `http://localhost:8080`
2. Find `hello_world` DAG
3. Toggle it ON (unpause)
4. Click "Trigger DAG" (play button)

**Via CLI:**
```bash
airflow dags trigger hello_world
```

### Step 4: Monitor execution

**In UI:**
- **Graph View:** See the DAG structure
- **Grid View:** See historical runs
- **Task Instance:** Click any task to see logs

**Via CLI:**
```bash
# List DAG runs
airflow dags list-runs -d hello_world

# View task logs
airflow tasks logs hello_world say_hello 2025-12-20
```

---

## ðŸ” Understanding DAG Parameters

| Parameter | Description | Example |
|-----------|-------------|---------|
| `dag_id` | Unique DAG identifier | `"hello_world"` |
| `start_date` | When DAG becomes active | `datetime(2025, 12, 1)` |
| `schedule_interval` | How often to run | `"@daily"`, `"0 6 * * *"` |
| `catchup` | Backfill past runs? | `False` (usually) |
| `tags` | Labels for organization | `["tutorial"]` |
| `default_args` | Shared task settings | `{"retries": 2}` |

---

## ðŸ“Š Exploring the UI

### Graph View
Shows DAG structure visually:
```
[hello] â”â”> [print_date] â”â”> [wait] â”â”> [goodbye]
```

### Grid View  
Shows run history:
```
Date       | hello | print_date | wait | goodbye
2025-12-20 |   âœ“   |     âœ“      |  âœ“   |    âœ“
2025-12-19 |   âœ“   |     âœ“      |  âœ“   |    âœ“
```

### Task Logs
Click any task to see:
- Stdout/stderr output
- Task duration
- Retry attempts
- Task instance details

---

## ðŸ› Common Issues

### Issue: "DAG not found"
**Cause:** Scheduler hasn't picked up the file yet  
**Solution:** Wait 30 seconds or restart scheduler

### Issue: "Import error"
**Cause:** Syntax error in DAG file  
**Solution:** Check `airflow dags list-import-errors`

### Issue: "Task stuck in queued"
**Cause:** Scheduler not running  
**Solution:** Start scheduler in separate terminal

### Issue: "Permission denied"
**Cause:** File permissions  
**Solution:** `chmod +x $AIRFLOW_HOME/dags/hello_world.py`

---

## âœï¸ Practice Exercise

**Create a data validation DAG:**

```python
from datetime import datetime
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

def validate_file():
    import os
    if os.path.exists('/tmp/data.csv'):
        print("âœ“ File exists")
        return True
    else:
        print("âœ— File missing")
        raise FileNotFoundError("data.csv not found")

with DAG(
    'data_validation_pipeline',
    start_date=datetime(2025, 12, 1),
    schedule_interval='@daily',
    catchup=False,
) as dag:
    
    create_file = BashOperator(
        task_id='create_sample_data',
        bash_command='echo "id,name,value\n1,test,100" > /tmp/data.csv',
    )
    
    check_file = PythonOperator(
        task_id='validate_file_exists',
        python_callable=validate_file,
    )
    
    process_file = BashOperator(
        task_id='process_data',
        bash_command='wc -l /tmp/data.csv',
    )
    
    cleanup = BashOperator(
        task_id='cleanup',
        bash_command='rm /tmp/data.csv',
    )
    
    create_file >> check_file >> process_file >> cleanup
```

**Test it:**
1. Save as `validation_dag.py` in dags folder
2. Trigger via UI
3. Verify all tasks succeed
4. Try making `validate_file()` fail and see what happens

---

## ðŸŽ¯ Key Takeaways

âœ… Airflow has 4 components: Webserver, Scheduler, Database, Executor  
âœ… DAGs are Python files in `$AIRFLOW_HOME/dags/`  
âœ… Use `>>` to define task dependencies  
âœ… UI provides Graph, Grid, and Log views  
âœ… `airflow dags` CLI commands for testing  

---

**Next:** `02_Day_02.md` - DAGs, Tasks & Scheduling in depth! ðŸ“…
