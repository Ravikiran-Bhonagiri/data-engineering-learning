# Day 3: Operators, Hooks & Sensors

**Time:** 1-2 hours  
**Goal:** Use Airflow's built-in operators and sensors for real integrations.

---

## ðŸ”§ Core Operators

### BashOperator
```python
from airflow.operators.bash import BashOperator

download = BashOperator(
    task_id='download_file',
    bash_command='curl -o /tmp/data.json https://api.example.com/data',
)
```

### PythonOperator
```python
from airflow.operators.python import PythonOperator

def process_data():
    print("Processing...")

process = PythonOperator(
    task_id='process',
    python_callable=process_data,
)
```

### EmailOperator
```python
from airflow.operators.email import EmailOperator

notify = EmailOperator(
    task_id='send_email',
    to='team@company.com',
    subject='Pipeline Complete',
    html_content='<p>Data loaded successfully</p>',
)
```

---

## ðŸ”Œ Database Operators

### PostgresOperator
```python
from airflow.providers.postgres.operators.postgres import PostgresOperator

create_table = PostgresOperator(
    task_id='create_staging_table',
    postgres_conn_id='my_postgres',
    sql="""
        CREATE TABLE IF NOT EXISTS staging_users (
            id INT,
            name VARCHAR(100),
            created_at TIMESTAMP
        );
    """,
)
```

### MySQLOperator
```python
from airflow.providers.mysql.operators.mysql import MySQLOperator

truncate = MySQLOperator(
    task_id='truncate_table',
    mysql_conn_id='my_mysql',
    sql='TRUNCATE TABLE staging_table;',
)
```

---

## ðŸª Hooks (Connection Abstraction)

### PostgresHook Example
```python
from airflow.providers.postgres.hooks.postgres import PostgresHook

def query_database():
    hook = PostgresHook(postgres_conn_id='my_postgres')
    
    # Execute query
    records = hook.get_records(
        "SELECT COUNT(*) FROM users WHERE active = true"
    )
    
    active_users = records[0][0]
    print(f"Active users: {active_users}")
    return active_users
```

**Hook benefits:**
- Centralized connection management
- Reusable authentication logic
- Connection pooling built-in

---

## ðŸ‘ï¸ Sensors (Wait for Conditions)

### FileSensor
```python
from airflow.sensors.filesystem import FileSensor

wait_for_file = FileSensor(
    task_id='wait_for_input',
    filepath='/tmp/input.csv',
    poke_interval=30,  # Check every 30 seconds
    timeout=3600,      # Fail after 1 hour
)
```

### ExternalTaskSensor
```python
from airflow.sensors.external_task import ExternalTaskSensor

wait_for_upstream = ExternalTaskSensor(
    task_id='wait_for_upstream_dag',
    external_dag_id='upstream_pipeline',
    external_task_id='final_task',
)
```

### DateTimeSensor
```python
from airflow.sensors.date_time import DateTimeSensor
from datetime import datetime

wait_until_9am = DateTimeSensor(
    task_id='wait_until_9am',
    target_time=datetime(2025, 12, 20, 9, 0, 0),
)
```

---

## ðŸŽ¯ Complete Example: Database ETL

```python
from datetime import datetime
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook

def extract_from_api():
    """Simulate API data extraction"""
    return [
        {'id': 1, 'name': 'Alice', 'amount': 100},
        {'id': 2, 'name': 'Bob', 'amount': 200},
    ]

def load_to_postgres(**context):
    """Load data using PostgresHook"""
    hook = PostgresHook(postgres_conn_id='my_postgres')
    
    # Get data from previous task (XCom)
    data = context['ti'].xcom_pull(task_ids='extract_data')
    
    # Insert rows
    for record in data:
        hook.run(
            "INSERT INTO staging_table (id, name, amount) VALUES (%s, %s, %s)",
            parameters=(record['id'], record['name'], record['amount'])
        )
    
    print(f"Loaded {len(data)} records")

with DAG(
    'postgres_etl',
    start_date=datetime(2025, 12, 1),
    schedule_interval='@daily',
    catchup=False,
) as dag:
    
    create_table = PostgresOperator(
        task_id='create_table',
        postgres_conn_id='my_postgres',
        sql="""
            CREATE TABLE IF NOT EXISTS staging_table (
                id INT PRIMARY KEY,
                name VARCHAR(100),
                amount DECIMAL(10,2),
                loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        """,
    )
    
    extract = PythonOperator(
        task_id='extract_data',
        python_callable=extract_from_api,
    )
    
    load = PythonOperator(
        task_id='load_data',
        python_callable=load_to_postgres,
    )
    
    create_table >> extract >> load
```

---

## ðŸŒ Cloud Operators

### S3ToRedshiftOperator
```python
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator

load_to_redshift = S3ToRedshiftOperator(
    task_id='load_s3_to_redshift',
    s3_bucket='my-bucket',
    s3_key='data/file.csv',
    schema='public',
    table='staging_table',
    copy_options=['CSV', 'IGNOREHEADER 1'],
)
```

### BigQueryInsertJobOperator
```python
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator

bq_job = BigQueryInsertJobOperator(
    task_id='run_bq_query',
    configuration={
        "query": {
            "query": "SELECT * FROM `project.dataset.table`",
            "useLegacySql": False,
        }
    },
)
```

---

## âš™ï¸ Setting Up Connections (UI)

1. Navigate to **Admin** â†’ **Connections**
2. Click **+** to add new connection
3. Configure:

**PostgreSQL Example:**
```
Connection Id: my_postgres
Connection Type: Postgres
Host: localhost
Schema: mydb
Login: postgres
Password: ****
Port: 5432
```

**AWS Example:**
```
Connection Id: aws_default
Connection Type: Amazon Web Services
AWS Access Key ID: AKIAXXXXX
AWS Secret Access Key: ****
Region Name: us-east-1
```

---

## âœï¸ Practice Exercise

**Build a sensor-based workflow:**

```python
from datetime import datetime
from airflow import DAG
from airflow.sensors.filesystem import FileSensor
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator

def process_file():
    print("Processing /tmp/input.csv...")
    # Your processing logic here

with DAG(
    'sensor_pipeline',
    start_date=datetime(2025, 12, 1),
    schedule_interval='@hourly',
    catchup=False,
) as dag:
    
    wait = FileSensor(
        task_id='wait_for_file',
        filepath='/tmp/input.csv',
        poke_interval=10,
        timeout=300,
    )
    
    process = PythonOperator(
        task_id='process_file',
        python_callable=process_file,
    )
    
    cleanup = BashOperator(
        task_id='cleanup',
        bash_command='rm /tmp/input.csv',
    )
    
    wait >> process >> cleanup
```

**Test it:**
```bash
# Create the file to trigger sensor
echo "test data" > /tmp/input.csv

# Trigger DAG
airflow dags trigger sensor_pipeline
```

---

## ðŸŽ¯ Key Takeaways

âœ… **Operators:** Execute actions (Bash, Python, SQL)  
âœ… **Hooks:** Manage connections (Postgres, S3, BigQuery)  
âœ… **Sensors:** Wait for conditions (files, time, external tasks)  
âœ… **Connections:** Configured in UI, reused across DAGs  
âœ… **XComs:** Pass data between tasks via `xcom_pull/push`  

---

**Next:** `02_Day_04.md` - Branching, XComs & Dynamic Workflows! ðŸŒ¿
