# Day 5: ETL Pipelines & External Integrations

**Time:** 1-2 hours  
**Goal:** Build production-ready ETL pipelines with real integrations.

---

## üîó Common Integrations

### AWS S3
```python
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

@task
def upload_to_s3():
    hook = S3Hook(aws_conn_id='aws_default')
    hook.load_file(
        filename='/tmp/data.csv',
        key='data/processed/data.csv',
        bucket_name='my-bucket',
        replace=True,
    )
```

### Google BigQuery
```python
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook

@task
def query_bigquery():
    hook = BigQueryHook(gcp_conn_id='gcp_default')
    sql = "SELECT COUNT(*) FROM `project.dataset.table`"
    records = hook.get_pandas_df(sql)
    return records
```

### REST APIs
```python
from airflow.providers.http.hooks.http import HttpHook

@task
def call_api():
    hook = HttpHook(http_conn_id='api_default', method='GET')
    response = hook.run('endpoint/data')
    return response.json()
```

---

## üéØ Complete ETL Example: API ‚Üí Transform ‚Üí Database

```python
from datetime import datetime
from airflow.decorators import dag, task
from airflow.providers.postgres.hooks.postgres import PostgresHook
import requests
import pandas as pd

@dag(
    'api_to_postgres_etl',
    start_date=datetime(2025, 12, 1),
    schedule='0 6 * * *',  # Daily at 6 AM
    catchup=False,
    default_args={'retries': 2},
)
def pipeline():
    
    @task
    def extract_from_api():
        """Extract data from REST API"""
        url = "https://jsonplaceholder.typicode.com/users"
        response = requests.get(url)
        response.raise_for_status()
        
        data = response.json()
        print(f"Extracted {len(data)} records")
        return data
    
    @task
    def transform_data(raw_data):
        """Clean and transform data"""
        df = pd.DataFrame(raw_data)
        
        # Select and rename columns
        df_clean = df[['id', 'name', 'email', 'phone']]
        df_clean.columns = ['user_id', 'full_name', 'email', 'phone']
        
        # Data quality checks
        df_clean = df_clean.dropna()
        df_clean = df_clean[df_clean['email'].str.contains('@')]
        
        print(f"Transformed to {len(df_clean)} clean records")
        return df_clean.to_dict('records')
    
    @task
    def load_to_postgres(data):
        """Load data to PostgreSQL"""
        hook = PostgresHook(postgres_conn_id='postgres_default')
        
        # Truncate staging table
        hook.run("TRUNCATE TABLE staging_users;")
        
        # Insert rows
        for record in data:
            hook.run(
                """
                INSERT INTO staging_users (user_id, full_name, email, phone)
                VALUES (%s, %s, %s, %s)
                """,
                parameters=(
                    record['user_id'],
                    record['full_name'],
                    record['email'],
                    record['phone']
                )
            )
        
        print(f"‚úÖ Loaded {len(data)} records to PostgreSQL")
    
    @task
    def validate_load():
        """Validate data was loaded correctly"""
        hook = PostgresHook(postgres_conn_id='postgres_default')
        
        count = hook.get_first("SELECT COUNT(*) FROM staging_users")[0]
        
        if count == 0:
            raise ValueError("No data loaded!")
        
        print(f"‚úÖ Validated {count} records in staging_users")
        return count
    
    # Define the flow
    raw = extract_from_api()
    clean = transform_data(raw)
    load_to_postgres(clean)
    validate_load()

etl_dag = pipeline()
```

---

## üì¶ Using Variables for Configuration

### Store config in Airflow Variables

```python
from airflow.models import Variable

@task
def use_variables():
    # Set via UI: Admin ‚Üí Variables
    api_key = Variable.get("api_key")
    api_url = Variable.get("api_url", default_var="https://default.com")
    
    # Use in code
    headers = {"Authorization": f"Bearer {api_key}"}
    # ...
```

---

## üîê Managing Secrets

### Use Connections for Credentials

```python
from airflow.hooks.base import BaseHook

@task
def get_credentials():
    # Get connection object
    conn = BaseHook.get_connection('my_service')
    
    username = conn.login
    password = conn.password
    host = conn.host
    
    # Use for authentication
    # ...
```

---

## ‚úèÔ∏è Practice Exercise

**Build a CSV ‚Üí S3 ‚Üí Redshift pipeline:**

```python
@dag('csv_to_redshift', start_date=datetime(2025, 12, 1), schedule='@daily')
def pipeline():
    
    @task
    def generate_csv():
        import csv
        data = [
            ['id', 'name', 'value'],
            [1, 'Alice', 100],
            [2, 'Bob', 200],
        ]
        
        with open('/tmp/data.csv', 'w') as f:
            writer = csv.writer(f)
            writer.writerows(data)
        
        return '/tmp/data.csv'
    
    @task
    def upload_to_s3(filepath):
        from airflow.providers.amazon.aws.hooks.s3 import S3Hook
        
        hook = S3Hook(aws_conn_id='aws_default')
        hook.load_file(
            filename=filepath,
            key='staging/data.csv',
            bucket_name='my-bucket',
        )
        
        return 's3://my-bucket/staging/data.csv'
    
    @task
    def copy_to_redshift(s3_path):
        from airflow.providers.amazon.aws.hooks.redshift_sql import RedshiftSQLHook
        
        hook = RedshiftSQLHook(redshift_conn_id='redshift_default')
        
        sql = f"""
        COPY staging_table
        FROM '{s3_path}'
        IAM_ROLE 'arn:aws:iam::123456789:role/RedshiftRole'
        CSV
        IGNOREHEADER 1;
        """
        
        hook.run(sql)
        print("‚úÖ Data copied to Redshift")
    
    file = generate_csv()
    s3_loc = upload_to_s3(file)
    copy_to_redshift(s3_loc)

dag = pipeline()
```

---

## üéØ Key Takeaways

‚úÖ **ETL Pattern:** Extract ‚Üí Transform ‚Üí Load  
‚úÖ **Hooks:** Manage external connections (S3, BigQuery, APIs)  
‚úÖ **Variables:** Store configuration  
‚úÖ **Connections:** Store credentials securely  
‚úÖ **Data Flow:** Use XCom for paths, external storage for data  

---

**Next:** `02_Day_06.md` - Production Best Practices & Debugging! üõ†Ô∏è
