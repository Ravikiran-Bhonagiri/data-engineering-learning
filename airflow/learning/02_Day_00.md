# Day 0: What is Airflow & Why It Matters

**Time:** 30-45 minutes  
**Goal:** Understand Airflow's value proposition with a real example.

---

##ðŸ” Before Airflow: Manual Scripts + Cron

Imagine a 3-step daily ETL process:

### Step 1: Extract
```bash
# Cron: 0 6 * * * /scripts/extract.sh
curl -o /tmp/daily_data.csv https://api.company.com/data?date=$(date +%Y-%m-%d)
```

### Step 2: Transform  
```bash
# Cron: 10 6 * * * /scripts/transform.sh
python /scripts/clean_data.py /tmp/daily_data.csv /tmp/clean_data.csv
```

### Step 3: Load
```bash
# Cron: 20 6 * * * /scripts/load.sh
psql -c "COPY staging_table FROM '/tmp/clean_data.csv' CSV;"
```

### Problems with this approach:

1. **Fragile timing:** If `extract` takes 15 minutes, `transform` runs on old data
2. **No retry logic:** One API timeout = broken pipeline
3. **No visibility:** Check 3 different log files to debug
4. **Manual backfills:** Need last week's data? Write custom script
5. **Dependency hell:** Add 10 more tasks? Maintain 10 cron entries

---

## âœ… With Airflow: Same Process as a DAG

```python
from datetime import datetime
from airflow.decorators import dag, task

@dag(
    dag_id="daily_etl",
    schedule="@daily",
    start_date=datetime(2025, 12, 1),
    catchup=False,
    default_args={"retries": 2},  # Auto-retry on failure
)
def daily_pipeline():
    
    @task
    def extract():
        """Fetch data from API"""
        import requests
        from datetime import datetime
        
        url = f"https://api.company.com/data?date={datetime.now().strftime('%Y-%m-%d')}"
        response = requests.get(url)
        data = response.json()
        
        # Return data (automatically passed to next task via XCom)
        return data
    
    @task
    def transform(raw_data):
        """Clean and validate data"""
        import pandas as pd
        
        df = pd.DataFrame(raw_data)
        df_clean = df.dropna()
        df_clean = df_clean[df_clean['amount'] > 0]  # Remove invalid records
        
        return df_clean.to_dict('records')
    
    @task
    def load(clean_data):
        """Load to PostgreSQL"""
        from airflow.providers.postgres.hooks.postgres import PostgresHook
        
        hook = PostgresHook(postgres_conn_id='my_postgres')
        hook.insert_rows(
            table='staging_table',
            rows=clean_data,
        )
        print(f"âœ… Loaded {len(clean_data)} records")
    
    # Define the flow (dependencies are automatic)
    raw = extract()
    cleaned = transform(raw)
    load(cleaned)

# Instantiate the DAG
dag = daily_pipeline()
```

### What improved:

| Problem | Solution |
|---------|----------|
| Timing issues | Dependencies managed automatically |
| No retries | `default_args={"retries": 2}` |
| No visibility | Web UI + logs in one place |
| Manual backfills | `airflow dags backfill` command |
| Dependency hell | One Python file, clear dependencies |

---

## ðŸŽ¯ Hands-On Example

### Simple DAG (Copy & Save as `test_dag.py`)

```python
from datetime import datetime
from airflow import DAG
from airflow.operators.bash import BashOperator

dag = DAG(
    dag_id="hello_airflow",
    start_date=datetime(2025, 12, 1),
    schedule="@daily",
    catchup=False,
)

say_hello = BashOperator(
    task_id="say_hello",
    bash_command="echo 'Hello from Airflow!'",
    dag=dag,
)

say_goodbye = BashOperator(
    task_id="say_goodbye",
    bash_command="echo 'Goodbye!'",
    dag=dag,
)

# Define order
say_hello >> say_goodbye
```

**What this does:**
1. Runs `say_hello` first
2. Then runs `say_goodbye`
3. Every day at midnight (`@daily`)

---

## ðŸ“Š Airflow UI Preview

When you run this DAG, the UI shows:

```
Graph View:
[say_hello] â”â”â”> [say_goodbye]
   âœ…              âœ…

Grid View:
2025-12-20  âœ… âœ…
2025-12-19  âœ… âœ…
2025-12-18  âœ… âœ…
```

Click on any task to see:
- Logs
- Duration
- Retries
- Task details

---

## ðŸš¦ Key Takeaways

1. **Airflow = Workflow Orchestration:** It schedules & monitors tasks
2. **DAGs = Workflows:** Directed Acyclic Graphs define task order
3. **Tasks = Units of Work:** Each task does one thing
4. **Dependencies:** `>>` or function calls define execution order
5. **Automatic Features:** Retries, logging, backfills, UI

---

## âœï¸ Practice Exercise

**Goal:** Understand the basics before setup.

1. **Draw a DAG on paper:**
   - Task A: Download file
   - Task B: Validate file
   - Task C: Upload to cloud
   - Task D: Send email notification
   
   **Constraints:** B depends on A, C depends on B, D depends on C

2. **Identify the failure points:**
   - What happens if Task A fails?
   - What happens if Task B succeeds but C fails?

3. **Think about retries:**
   - Which tasks should retry automatically?
   - Which tasks should NOT retry (e.g., sending duplicate emails)?

**Answer:**
```python
# Dependencies:
A >> B >> C >> D

# If A fails: B, C, D won't run (dependencies not met)
# If C fails: D won't run, but B keeps its success state
# Retry: A, B, C (yes), D (no, to avoid duplicate emails)
```

---

**Next:** `02_Day_01.md` - Install Airflow and run your first DAG! ðŸ”§
