# Day 6: Production Best Practices & Debugging

**Time:** 1-2 hours  
**Goal:** Deploy Airflow to production and debug common issues.

---

## üèóÔ∏è Production Setup

### Executor Choice

| Executor | Use Case | Pros | Cons |
|----------|----------|------|------|
| **SequentialExecutor** | Local dev | Simple setup | Single task at a time |
| **LocalExecutor** | Small prod | No queue needed | Single machine |
| **CeleryExecutor** | Large prod | Distributed | Needs Redis/RabbitMQ |
| **KubernetesExecutor** | Cloud-native | Auto-scaling | K8s complexity |

### Recommended Stack

```
Production:
‚îú‚îÄ‚îÄ Database: PostgreSQL (not SQLite!)
‚îú‚îÄ‚îÄ Executor: Celery or Kubernetes
‚îú‚îÄ‚îÄ Queue: Redis or RabbitMQ (for Celery)
‚îú‚îÄ‚îÄ Storage: S3/GCS for logs
‚îî‚îÄ‚îÄ Monitoring: Prometheus + Grafana
```

---

## üìù Logging Best Practices

### Log to External Storage

```python
# airflow.cfg
[logging]
remote_logging = True
remote_base_log_folder = s3://my-bucket/airflow/logs
remote_log_conn_id = aws_default
```

### Custom Logging in Tasks

```python
@task
def my_task():
    import logging
    logger = logging.getLogger(__name__)
    
    logger.info("Starting task")
    logger.warning("This is a warning")
    logger.error("This is an error")
    
    # Logs appear in Airflow UI automatically
```

---

## üîç Debugging Common Issues

### Issue 1: Task Stuck in "Queued"

**Cause:** No workers available  
**Solution:**
```bash
# Check workers
airflow celery worker  # Start a worker

# Or increase parallelism
# airflow.cfg: parallelism = 32
```

### Issue 2: Import Error

**Cause:** Syntax error in DAG file  
**Debug:**
```bash
# Check for import errors
airflow dags list-import-errors

# Test DAG manually
python $AIRFLOW_HOME/dags/my_dag.py
```

### Issue 3: Task Fails Silently

**Cause:** Exception not raised properly  
**Solution:**
```python
@task
def my_task():
    try:
        # risky operation
        result = 1 / 0
    except Exception as e:
        # Log AND re-raise
        logging.error(f"Task failed: {e}")
        raise  # Don't swallow the exception!
```

### Issue 4: DAG Takes Forever to Schedule

**Cause:** Too many past runs to check  
**Solution:**
```python
# Use catchup=False
@dag(catchup=False, ...)

# Or limit backfill
max_active_runs=1
```

---

## ‚öôÔ∏è Configuration Tuning

### Key airflow.cfg Settings

```ini
[core]
parallelism = 32                    # Max tasks across all DAGs
max_active_runs_per_dag = 16        # Max concurrent runs per DAG
dag_concurrency = 16                # Max tasks per DAG run
load_examples = False               # Don't load example DAGs

[scheduler]
min_file_process_interval = 30      # Scan DAGs every 30s
dag_dir_list_interval = 300         # Scan new files every 5min

[webserver]
expose_config = False               # Security: hide config
web_server_worker_timeout = 120     # Timeout for UI
```

---

## üß™ Testing DAGs

### Unit Tests

```python
# test_my_dag.py
from airflow.models import DagBag

def test_dag_loads_without_errors():
    dag_bag = DagBag(dag_folder='dags/', include_examples=False)
    assert len(dag_bag.import_errors) == 0, f"Errors: {dag_bag.import_errors}"

def test_dag_has_correct_tasks():
    dag_bag = DagBag(dag_folder='dags/')
    dag = dag_bag.get_dag('my_dag')
    
    assert len(dag.tasks) == 4
    assert 'extract' in dag.task_ids
```

### Test Individual Tasks

```python
from airflow.models import DagBag
from datetime import datetime

dag_bag = DagBag()
dag = dag_bag.get_dag('my_dag')

# Get task
task = dag.get_task('extract')

# Execute
task.execute(context={'ds': '2025-12-20'})
```

---

## üìä Monitoring & Alerts

### Email Alerts on Failure

```python
from datetime import datetime
from airflow.utils.email import send_email

def notify_failure(context):
    """Send email on task failure"""
    send_email(
        to='data-team@company.com',
        subject=f"Task Failed: {context['task_instance'].task_id}",
        html_content=f"""
        <h3>Task Failed</h3>
        <p>DAG: {context['dag'].dag_id}</p>
        <p>Task: {context['task_instance'].task_id}</p>
        <p>Date: {context['execution_date']}</p>
        <p>Log: {context['task_instance'].log_url}</p>
        """
    )

@dag(
    default_args={
        'on_failure_callback': notify_failure,
    },
    ...
)
```

### Slack Notifications

```python
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator

notify_slack = SlackWebhookOperator(
    task_id='notify_slack',
    slack_webhook_conn_id='slack_webhook',
    message='Pipeline completed successfully!',
)
```

---

## üîê Security Best Practices

### 1. Use Secrets Backend

```python
# airflow.cfg
[secrets]
backend = airflow.providers.amazon.aws.secrets.systems_manager.SystemsManagerParameterStoreBackend
backend_kwargs = {"connections_prefix": "/airflow/connections"}
```

### 2. RBAC (Role-Based Access Control)

```python
# Enable RBAC in webserver
[webserver]
rbac = True
```

### 3. Encrypt Connections

```python
# Set fernet key for encryption
[core]
fernet_key = YOUR_FERNET_KEY_HERE
```

---

## üìà Performance Optimization

### 1. Use Pools

```python
# Limit parallel tasks for external API
@task(pool='api_pool', pool_slots=1)
def call_api():
    # ...
```

### 2. Use Task Priority

```python
@task(priority_weight=10)  # Higher = more important
def critical_task():
    # ...
```

### 3. Use Deferrable Operators

```python
from airflow.sensors.filesystem import FileSensor

wait = FileSensor(
    task_id='wait',
    filepath='/tmp/file.csv',
    deferrable=True,  # Frees worker while waiting
)
```

---

## ‚úèÔ∏è Practice Exercise

**Add comprehensive monitoring:**

```python
from airflow.decorators import dag, task
from datetime import datetime
import logging

def task_failure_alert(context):
    logging.error(f"ALERT: {context['task_instance'].task_id} failed!")

@dag(
    'monitored_pipeline',
    start_date=datetime(2025, 12, 1),
    schedule='@daily',
    catchup=False,
    default_args={
        'retries': 2,
        'on_failure_callback': task_failure_alert,
        'email_on_failure': True,
        'email': ['alerts@company.com'],
    },
)
def pipeline():
    
    @task
    def extract():
        logging.info("Extracting data...")
        return [1, 2, 3]
    
    @task
    def transform(data):
        logging.info(f"Transforming {len(data)} records")
        return [x * 2 for x in data]
    
    @task
    def load(data):
        logging.info(f"Loading {len(data)} records")
        # Simulate failure 50% of time for testing
        import random
        if random.choice([True, False]):
            raise ValueError("Simulated failure!")
    
    data = extract()
    transformed = transform(data)
    load(transformed)

dag = pipeline()
```

---

## üéØ Production Checklist

- [ ] Use PostgreSQL or MySQL (not SQLite)
- [ ] Set `catchup=False` on new DAGs
- [ ] Configure email alerts
- [ ] Use secrets backend for credentials
- [ ] Enable RBAC
- [ ] Set up log rotation or remote logging
- [ ] Monitor scheduler health
- [ ] Use pools for rate limiting
- [ ] Write unit tests for DAGs
- [ ] Document DAG purpose in description

---

## üéØ Key Takeaways

‚úÖ **Production:** Use Celery/K8s Executor + PostgreSQL  
‚úÖ **Debugging:** Check logs, import errors, worker status  
‚úÖ **Monitoring:** Email/Slack alerts, logging  
‚úÖ **Security:** Secrets backend, RBAC, encryption  
‚úÖ **Performance:** Pools, priority, deferrable operators  

---

**Next:** `03_QUICK_Reference.md` - Command cheatsheet & interview prep! üìã
