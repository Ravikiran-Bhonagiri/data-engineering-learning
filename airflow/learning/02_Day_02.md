# Day 2: DAGs, Tasks & Scheduling

**Time:** 1-2 hours  
**Goal:** Master DAG authoring using both classic and TaskFlow API.

---

## ðŸ“š Two Ways to Define DAGs

### Method 1: Classic DAG Context Manager

```python
from datetime import datetime
from airflow import DAG
from airflow.operators.python import PythonOperator

def my_function():
    print("Hello from classic DAG")

with DAG(
    'classic_example',
    start_date=datetime(2025, 12, 1),
    schedule_interval='@daily',
) as dag:
    task = PythonOperator(
        task_id='run_function',
        python_callable=my_function,
    )
```

### Method 2: TaskFlow API (Recommended)

```python
from datetime import datetime
from airflow.decorators import dag, task

@dag(
    dag_id='taskflow_example',
    start_date=datetime(2025, 12, 1),
    schedule='@daily',
)
def my_pipeline():
    @task
    def my_function():
        print("Hello from TaskFlow API")
        return "some_value"
    
    my_function()

dag_instance = my_pipeline()
```

**Differences:**
- TaskFlow is cleaner, less boilerplate
- Automatic XCom handling (data passing)
- Better type hints support

---

## ðŸ•°ï¸ Scheduling Options

### Cron Expressions
```python
schedule_interval='0 6 * * *'     # Daily at 6 AM
schedule_interval='*/15 * * * *'  # Every 15 minutes
schedule_interval='0 0 * * 0'     # Weekly on Sunday
schedule_interval='0 0 1 * *'     # Monthly on 1st
```

### Presets
```python
schedule_interval='@once'      # Run only once
schedule_interval='@hourly'    # 0 * * * *
schedule_interval='@daily'     # 0 0 * * *
schedule_interval='@weekly'    # 0 0 * * 0
schedule_interval='@monthly'   # 0 0 1 * *
schedule_interval='@yearly'    # 0 0 1 1 *
schedule_interval=None         # Manual trigger only
```

### Timedelta
```python
from datetime import timedelta

schedule_interval=timedelta(hours=4)  # Every 4 hours
schedule_interval=timedelta(days=1)   # Daily
```

---

## ðŸ”— Task Dependencies

### Sequential
```python
task1 >> task2 >> task3
# Equivalent to:
# task1.set_downstream(task2)
# task2.set_downstream(task3)
```

### Parallel then Merge
```python
task1 >> [task2, task3] >> task4
#    task1
#    /    \
# task2  task3
#    \    /
#    task4
```

### Fan-out / Fan-in
```python
[task1, task2] >> task3 >> [task4, task5]
```

---

## ðŸŽ¯ Complete Example: ETL Pipeline

```python
from datetime import datetime
from airflow.decorators import dag, task

@dag(
    dag_id='etl_pipeline',
    start_date=datetime(2025, 12, 1),
    schedule='0 6 * * *',  # 6 AM daily
    catchup=False,
    default_args={
        'retries': 2,
        'retry_delay': timedelta(minutes=5),
    },
    tags=['production', 'etl'],
)
def pipeline():
    
    @task
    def extract():
        """Extract data from source"""
        data = [
            {'id': 1, 'value': 100},
            {'id': 2, 'value': 200},
            {'id': 3, 'value': 300},
        ]
        print(f"Extracted {len(data)} records")
        return data
    
    @task
    def transform(data):
        """Apply business logic"""
        transformed = [
            {**record, 'value': record['value'] * 2}
            for record in data
        ]
        print(f"Transformed {len(transformed)} records")
        return transformed
    
    @task
    def load(data):
        """Load to destination"""
        print(f"Loading {len(data)} records...")
        for record in data:
            print(f"  Record {record['id']}: {record['value']}")
        return len(data)
    
    @task
    def send_notification(count):
        """Send success notification"""
        print(f"âœ… Pipeline complete! Loaded {count} records")
    
    # Define the flow
    raw_data = extract()
    clean_data = transform(raw_data)
    record_count = load(clean_data)
    send_notification(record_count)

etl_dag = pipeline()
```

**What this demonstrates:**
- âœ… Task dependencies (automatic from function calls)
- âœ… Data passing (via return values/XCom)
- âœ… Retry logic (via `default_args`)
- âœ… Tags for organization

---

## ðŸ“… Important DAG Parameters

### Start Date & Catchup

```python
@dag(
    start_date=datetime(2025, 1, 1),
    catchup=True,  # <-- Backfill past runs
)
```

**Example:**
- Today: 2025-12-20
- `start_date`: 2025-01-01
- `schedule`: `@daily`
- `catchup=True`: Airflow runs ALL days from Jan 1 to Dec 20
- `catchup=False`: Only runs from today forward

**Best practice:** Use `catchup=False` for learning.

---

### Default Args (Task-Level Config)

```python
from datetime import timedelta

default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'email': ['alerts@company.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(minutes=30),
}

@dag(default_args=default_args, ...)
```

---

## ðŸ”„ Dynamic Task Generation

### Using a loop (Classic)

```python
with DAG('dynamic_example', ...) as dag:
    for i in range(5):
        BashOperator(
            task_id=f'task_{i}',
            bash_command=f'echo "Processing {i}"',
        )
```

### Using expand (TaskFlow)

```python
@dag(...)
def pipeline():
    @task
    def get_items():
        return ['file1.csv', 'file2.csv', 'file3.csv']
    
    @task
    def process_file(filename):
        print(f"Processing {filename}")
    
    # Dynamic mapping - creates 3 tasks
    process_file.expand(filename=get_items())
```

---

## âœï¸ Practice Exercise

**Create a multi-stage data quality DAG:**

```python
from datetime import datetime
from airflow.decorators import dag, task

@dag(
    'data_quality_pipeline',
    start_date=datetime(2025, 12, 1),
    schedule='@daily',
    catchup=False,
)
def pipeline():
    
    @task
    def check_row_count():
        count = 1000  # Simulated
        if count < 100:
            raise ValueError("Too few rows!")
        return count
    
    @task
    def check_null_values():
        null_pct = 5  # Simulated
        if null_pct > 10:
            raise ValueError("Too many nulls!")
        return null_pct
    
    @task
    def check_duplicates():
        dup_count = 0  # Simulated
        if dup_count > 0:
            raise ValueError(f"Found {dup_count} duplicates!")
        return dup_count
    
    @task
    def mark_as_validated(row_count, null_pct, dup_count):
        print(f"âœ… Data validated:")
        print(f"  Rows: {row_count}")
        print(f"  Null %: {null_pct}")
        print(f"  Duplicates: {dup_count}")
    
    # Run all checks in parallel, then validate
    rows = check_row_count()
    nulls = check_null_values()
    dups = check_duplicates()
    
    mark_as_validated(rows, nulls, dups)

dag = pipeline()
```

**Task structure:**
```
    check_row_count â”€â”€â”
                      â”œâ”€â”€> mark_as_validated
    check_null_values â”¤
                      â”‚
    check_duplicates â”€â”˜
```

---

## ðŸŽ¯ Key Takeaways

âœ… **Two APIs:** Classic (`with DAG`) vs TaskFlow (`@dag`)  
âœ… **Scheduling:** Cron, presets, or timedelta  
âœ… **Dependencies:** `>>`, `<<`, or function calls  
âœ… **Catchup:** Usually set to `False` in dev  
âœ… **Default args:** Share config across tasks  
âœ… **Dynamic tasks:** Use `.expand()` for parallelization  

---

**Next:** `02_Day_03.md` - Operators, Hooks & Sensors! ðŸ”Œ
