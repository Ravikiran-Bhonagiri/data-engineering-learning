# PySpark Foundation

**For:** Python developers who want to process big data  
**Goal:** Understand what PySpark is, why it matters, and how Spark works.

---

## ğŸ¤” The Problem: Python's Single-Machine Limitation

### Before PySpark: Pandas on One Machine

```python
import pandas as pd

# Works great for small data
df = pd.read_csv("sales_data.csv")  # 100 MB file
result = df.groupby("region").agg({"revenue": "sum"})
print(result)
```

**But what happens with 100 GB or 1 TB of data?**

```python
# This will crash your laptop!
df = pd.read_csv("massive_sales_data.csv")  # 100 GB file
# MemoryError: Unable to allocate 95.4 GiB
```

### Problems with Single-Machine Python:
1. **Memory limits:** Data must fit in RAM
2. **No parallelism:** Single CPU core processing
3. **No fault tolerance:** Crash = start over
4. **No scalability:** Can't add more machines

---

## âœ… The Solution: Apache Spark + PySpark

**PySpark** is the Python API for Apache Spark, a distributed computing engine that:
- Processes data **across clusters** of machines
- Runs **in-memory** for speed (100x faster than Hadoop MapReduce)
- Provides **fault tolerance** (recovers from failures automatically)
- Scales from **laptop to thousands of nodes**

### Same analysis in PySpark:

```python
from pyspark.sql import SparkSession
import pyspark.sql.functions as F

# Create Spark session
spark = SparkSession.builder.appName("sales_analysis").getOrCreate()

# Read 100 GB file distributed across cluster
df = spark.read.csv("s3://data/massive_sales_data.csv", header=True)

# Process across 100+ machines in parallel
result = df.groupBy("region").agg(F.sum("revenue").alias("total_revenue"))

result.show()
```

**What changed:**
- âœ… **No memory limit:** Data distributed across cluster
- âœ… **Parallel processing:** Uses all CPU cores across all machines
- âœ… **Fault tolerance:** Automatic recovery from failures
- âœ… **Scalable:** Add machines to handle more data

---

## ğŸ—ï¸ Spark Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DRIVER PROGRAM                    â”‚
â”‚  (Your Python code runs here - SparkSession)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CLUSTER MANAGER                     â”‚
â”‚        (YARN, Kubernetes, Mesos, Standalone)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EXECUTOR 1  â”‚ â”‚   EXECUTOR 2  â”‚ â”‚   EXECUTOR 3  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Task 1  â”‚  â”‚ â”‚  â”‚ Task 3  â”‚  â”‚ â”‚  â”‚ Task 5  â”‚  â”‚
â”‚  â”‚ Task 2  â”‚  â”‚ â”‚  â”‚ Task 4  â”‚  â”‚ â”‚  â”‚ Task 6  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚   [Cache]     â”‚ â”‚   [Cache]     â”‚ â”‚   [Cache]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components:

| Component | Role |
|-----------|------|
| **Driver** | Your code; coordinates execution |
| **Cluster Manager** | Allocates resources (YARN, K8s) |
| **Executors** | JVM processes that run tasks |
| **Tasks** | Units of work on data partitions |
| **Cache** | In-memory storage for speed |

---

## ğŸ“¦ PySpark Ecosystem

| Module | Purpose | Use Case |
|--------|---------|----------|
| **Spark SQL & DataFrames** | Structured data processing | ETL, analytics, SQL queries |
| **Structured Streaming** | Real-time data processing | IoT, logs, event streams |
| **MLlib** | Machine learning at scale | Training models on big data |
| **Spark Core (RDDs)** | Low-level API | Custom transformations |
| **Pandas API on Spark** | Pandas-like syntax | Easy migration from pandas |

---

## ğŸ†š When to Use/Not Use PySpark

### âœ… Use PySpark For:

| Use Case | Why PySpark? |
|----------|--------------|
| **Big data ETL** | Process TB/PB of data |
| **Data lake analytics** | Query Parquet, Delta Lake |
| **ML on large datasets** | Train models on billions of rows |
| **Real-time streaming** | Process Kafka, Kinesis streams |
| **Data warehouse queries** | Replace expensive cloud DW |

### âŒ Don't Use PySpark For:

| Use Case | Why Not? | Alternative |
|----------|----------|-------------|
| **Small data (<10 GB)** | Overkill, slow startup | Pandas, Polars |
| **Real-time (<100ms)** | Latency too high | Flink, Kafka Streams |
| **Simple scripts** | Too complex | Python + pandas |
| **Interactive notebooks** | Slow iteration | Pandas, DuckDB |

**Rule of thumb:** If data fits in memory â†’ use pandas. If not â†’ use PySpark.

---

## ğŸ“Š PySpark vs. Alternatives

| Tool | Best For | Distributed? |
|------|----------|--------------|
| **Pandas** | Small data, quick analysis | âŒ Single machine |
| **Polars** | Medium data, fast | âŒ Single machine |
| **DuckDB** | Analytics, SQL | âŒ Single machine |
| **Dask** | Pandas at scale | âœ… Limited |
| **PySpark** | Big data, production | âœ… Full cluster |
| **Snowflake/BigQuery** | Managed analytics | âœ… Cloud only |

---

## ğŸ“ Key Concepts

### RDD (Resilient Distributed Dataset)
- Low-level API (foundational)
- Immutable, partitioned collection
- Fault-tolerant via lineage

### DataFrame
- High-level API (recommended)
- Schema-based like SQL table
- Optimized by Catalyst query optimizer

### Transformations vs Actions
```python
# Transformations (lazy - don't execute immediately)
df.filter(df.age > 21)      # Returns new DataFrame
df.select("name", "age")    # Returns new DataFrame

# Actions (trigger execution)
df.count()                  # Returns number
df.show()                   # Prints rows
df.collect()                # Returns all rows to driver
```

### Lazy Evaluation
```python
# Nothing happens yet (just builds plan)
df2 = df.filter(df.age > 21)
df3 = df2.select("name")

# NOW Spark executes the entire plan
df3.show()  # Triggers execution
```

---

## ğŸš€ What You'll Build

By the end of this course:
- âœ… Set up PySpark locally and on clusters
- âœ… Process large datasets with DataFrames
- âœ… Write SQL queries on Spark
- âœ… Build streaming applications
- âœ… Train ML models at scale
- âœ… Optimize Spark jobs for performance
- âœ… Deploy to production (Databricks, EMR)

**Next Step:** Open `02_Day_00.md` to start! ğŸ¯
