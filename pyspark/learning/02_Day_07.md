# Day 7: ML Pipelines & Feature Engineering

**Time:** 1-2 hours  
**Goal:** Build end-to-end ML pipelines with proper feature engineering.

---

## ðŸ”— What is a Pipeline?

A **Pipeline** chains multiple stages (transformers + estimators) into a single, reusable workflow.

```
[Data] â†’ [Indexer] â†’ [Assembler] â†’ [Scaler] â†’ [Classifier] â†’ [Predictions]
```

---

## ðŸ“¦ Building a Pipeline

```python
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.feature import (
    StringIndexer, VectorAssembler, StandardScaler
)
from pyspark.ml.classification import RandomForestClassifier

spark = SparkSession.builder.appName("pipelines").getOrCreate()

# Sample data
data = spark.createDataFrame([
    ("male", 22.0, 7.25, 0),
    ("female", 38.0, 71.28, 1),
    ("female", 26.0, 7.92, 1),
    ("male", 35.0, 53.10, 0),
], ["gender", "age", "fare", "survived"])

# Define pipeline stages
gender_indexer = StringIndexer(inputCol="gender", outputCol="gender_idx")
assembler = VectorAssembler(
    inputCols=["gender_idx", "age", "fare"],
    outputCol="features"
)
scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
classifier = RandomForestClassifier(
    featuresCol="scaled_features",
    labelCol="survived",
    numTrees=10
)

# Build pipeline
pipeline = Pipeline(stages=[
    gender_indexer,
    assembler,
    scaler,
    classifier
])

# Fit pipeline (trains all stages)
train, test = data.randomSplit([0.8, 0.2], seed=42)
model = pipeline.fit(train)

# Predict
predictions = model.transform(test)
predictions.select("gender", "age", "survived", "prediction").show()
```

---

## ðŸ’¾ Save and Load Pipelines

```python
# Save trained model
model.save("/models/survival_model")

# Load later
from pyspark.ml import PipelineModel
loaded_model = PipelineModel.load("/models/survival_model")

# Use loaded model
predictions = loaded_model.transform(new_data)
```

---

## ðŸ”§ Feature Engineering Techniques

### Handle Missing Values

```python
from pyspark.ml.feature import Imputer

imputer = Imputer(
    inputCols=["age", "fare"],
    outputCols=["age_imputed", "fare_imputed"],
    strategy="mean"  # or "median", "mode"
)
df = imputer.fit(df).transform(df)
```

### Bucketizer (Binning)

```python
from pyspark.ml.feature import Bucketizer

bucketizer = Bucketizer(
    splits=[0, 18, 35, 60, 100],
    inputCol="age",
    outputCol="age_bucket"
)
df = bucketizer.transform(df)
# age_bucket: 0 = <18, 1 = 18-35, 2 = 35-60, 3 = 60+
```

### QuantileDiscretizer

```python
from pyspark.ml.feature import QuantileDiscretizer

discretizer = QuantileDiscretizer(
    numBuckets=4,
    inputCol="fare",
    outputCol="fare_quartile"
)
df = discretizer.fit(df).transform(df)
```

### Polynomial Features

```python
from pyspark.ml.feature import PolynomialExpansion

poly = PolynomialExpansion(
    degree=2,
    inputCol="features",
    outputCol="poly_features"
)
df = poly.transform(df)
```

### PCA (Dimensionality Reduction)

```python
from pyspark.ml.feature import PCA

pca = PCA(k=3, inputCol="features", outputCol="pca_features")
model = pca.fit(df)
df = model.transform(df)
```

---

## ðŸŽ¯ Hyperparameter Tuning

### CrossValidator

```python
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Define parameter grid
paramGrid = ParamGridBuilder() \
    .addGrid(classifier.numTrees, [10, 50, 100]) \
    .addGrid(classifier.maxDepth, [3, 5, 10]) \
    .build()

# Cross-validator
cv = CrossValidator(
    estimator=pipeline,
    estimatorParamMaps=paramGrid,
    evaluator=BinaryClassificationEvaluator(labelCol="survived"),
    numFolds=3
)

# Find best model
best_model = cv.fit(train)
print(f"Best params: {best_model.bestModel.stages[-1].getNumTrees}")
```

### TrainValidationSplit (Faster)

```python
from pyspark.ml.tuning import TrainValidationSplit

tvs = TrainValidationSplit(
    estimator=pipeline,
    estimatorParamMaps=paramGrid,
    evaluator=BinaryClassificationEvaluator(labelCol="survived"),
    trainRatio=0.8
)
best_model = tvs.fit(train)
```

---

## ðŸ“Š Complete Pipeline Example

```python
from pyspark.ml import Pipeline
from pyspark.ml.feature import (
    StringIndexer, VectorAssembler, StandardScaler, Imputer
)
from pyspark.ml.classification import GBTClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Full pipeline
pipeline = Pipeline(stages=[
    # Handle missing values
    Imputer(inputCols=["age", "fare"], outputCols=["age_imp", "fare_imp"]),
    
    # Encode categoricals
    StringIndexer(inputCol="gender", outputCol="gender_idx"),
    StringIndexer(inputCol="embarked", outputCol="embarked_idx"),
    
    # Assemble features
    VectorAssembler(
        inputCols=["gender_idx", "embarked_idx", "age_imp", "fare_imp", "pclass"],
        outputCol="features"
    ),
    
    # Scale features
    StandardScaler(inputCol="features", outputCol="scaled"),
    
    # Classifier
    GBTClassifier(featuresCol="scaled", labelCol="survived", maxIter=20)
])

# Train
model = pipeline.fit(train_df)

# Evaluate
predictions = model.transform(test_df)
evaluator = BinaryClassificationEvaluator(labelCol="survived")
print(f"AUC: {evaluator.evaluate(predictions)}")

# Save
model.save("/models/titanic_gbt_v1")
```

---

## ðŸ§ª Practice Exercise

```python
# Build a pipeline for customer churn prediction:
# 1. Impute missing values in "tenure" and "monthly_charges"
# 2. Index categorical columns: "contract", "payment_method"
# 3. Assemble all features
# 4. Train RandomForestClassifier
# 5. Use CrossValidator to find best numTrees (10, 50, 100)
# 6. Evaluate with AUC
```

---

## ðŸŽ¯ Key Takeaways

âœ… **Pipeline:** Chain transformers + estimators  
âœ… **Save/Load:** Persist trained pipelines  
âœ… **Imputer:** Handle missing values  
âœ… **Bucketizer:** Bin continuous variables  
âœ… **CrossValidator:** Hyperparameter tuning  
âœ… **StandardScaler:** Normalize features  

---

**Next:** `02_Day_08.md` - Delta Lake & Data Lakehouse! ðŸ 
