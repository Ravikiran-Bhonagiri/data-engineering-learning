# Day 4: Spark SQL

**Time:** 1-2 hours  
**Goal:** Use SQL to query DataFrames and optimize queries.

---

## üóÉÔ∏è Why Spark SQL?

Spark SQL lets you:
- Query DataFrames using SQL syntax
- Mix SQL with Python code
- Leverage Catalyst optimizer automatically
- Connect to Hive, JDBC, and other data sources

---

## üìù Creating Temporary Views

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("spark_sql").getOrCreate()

# Create DataFrame
employees = spark.createDataFrame([
    (1, "Alice", "Engineering", 120000),
    (2, "Bob", "Sales", 80000),
    (3, "Charlie", "Engineering", 95000),
], ["id", "name", "dept", "salary"])

# Register as temporary view
employees.createOrReplaceTempView("employees")

# Now you can use SQL!
result = spark.sql("SELECT * FROM employees WHERE salary > 90000")
result.show()
```

---

## üîç SQL Queries

### Basic SELECT

```python
spark.sql("SELECT name, salary FROM employees").show()

spark.sql("""
    SELECT name, salary, salary * 0.1 AS bonus
    FROM employees
    WHERE dept = 'Engineering'
""").show()
```

### Aggregations

```python
spark.sql("""
    SELECT 
        dept,
        COUNT(*) AS emp_count,
        AVG(salary) AS avg_salary,
        MAX(salary) AS max_salary
    FROM employees
    GROUP BY dept
    ORDER BY avg_salary DESC
""").show()
```

### Joins

```python
# Create departments table
departments = spark.createDataFrame([
    ("Engineering", "Building A"),
    ("Sales", "Building B"),
], ["dept", "location"])
departments.createOrReplaceTempView("departments")

# SQL join
spark.sql("""
    SELECT e.name, e.salary, d.location
    FROM employees e
    JOIN departments d ON e.dept = d.dept
""").show()
```

---

## ü™ü Window Functions

```python
spark.sql("""
    SELECT 
        name,
        dept,
        salary,
        ROW_NUMBER() OVER (PARTITION BY dept ORDER BY salary DESC) as rank,
        AVG(salary) OVER (PARTITION BY dept) as dept_avg,
        salary - AVG(salary) OVER (PARTITION BY dept) as diff_from_avg
    FROM employees
""").show()
```

---

## üìä Common SQL Functions

```python
# String functions
spark.sql("SELECT UPPER(name), LENGTH(name) FROM employees").show()

# Date functions
spark.sql("""
    SELECT 
        CURRENT_DATE() as today,
        DATE_ADD(CURRENT_DATE(), 30) as in_30_days,
        YEAR(CURRENT_DATE()) as year
""").show()

# Conditional
spark.sql("""
    SELECT 
        name,
        CASE 
            WHEN salary > 100000 THEN 'High'
            WHEN salary > 70000 THEN 'Medium'
            ELSE 'Low'
        END as salary_tier
    FROM employees
""").show()
```

---

## üí° Query Optimization

### Explain Plan

```python
# See how Spark will execute the query
df = spark.sql("SELECT * FROM employees WHERE salary > 90000")
df.explain()        # Simple plan
df.explain(True)    # Extended plan
```

### Broadcast Join Hint

```python
spark.sql("""
    SELECT /*+ BROADCAST(d) */ e.name, d.location
    FROM employees e
    JOIN departments d ON e.dept = d.dept
""").show()
```

### Caching

```python
spark.sql("CACHE TABLE employees")
spark.sql("UNCACHE TABLE employees")
```

---

## üìÅ Reading from External Sources

### Hive Tables

```python
spark = SparkSession.builder \
    .appName("hive_example") \
    .enableHiveSupport() \
    .getOrCreate()

spark.sql("SHOW DATABASES").show()
spark.sql("USE my_database")
spark.sql("SELECT * FROM my_table LIMIT 10").show()
```

### JDBC (PostgreSQL, MySQL)

```python
df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:5432/mydb") \
    .option("dbtable", "public.sales") \
    .option("user", "username") \
    .option("password", "password") \
    .load()

df.show()
```

---

## üìù Creating Permanent Tables

```python
# Save as Parquet table
employees.write.saveAsTable("analytics.employees")

# With partitioning
employees.write \
    .partitionBy("dept") \
    .saveAsTable("analytics.employees_partitioned")

# Overwrite
employees.write \
    .mode("overwrite") \
    .saveAsTable("analytics.employees")
```

---

## üß™ Practice Exercise

```python
# Create sales data
sales = spark.createDataFrame([
    ("2025-01-01", "Electronics", 1000, 5),
    ("2025-01-01", "Clothing", 500, 10),
    ("2025-01-02", "Electronics", 1500, 3),
    ("2025-01-02", "Clothing", 300, 8),
    ("2025-01-03", "Electronics", 800, 4),
], ["date", "category", "revenue", "quantity"])
sales.createOrReplaceTempView("sales")

# TODO: Write SQL queries to:
# 1. Total revenue per category
# 2. Daily revenue with running total
# 3. Top category by revenue each day (window function)
```

---

## üéØ Key Takeaways

‚úÖ **`createOrReplaceTempView`:** Register DataFrame for SQL  
‚úÖ **`spark.sql()`:** Execute SQL queries  
‚úÖ **Window functions:** Advanced analytics  
‚úÖ **`explain()`:** See query plan  
‚úÖ **JDBC:** Connect to external databases  
‚úÖ **Catalyst optimizer:** Automatic query optimization  

---

**Next:** `02_Day_05.md` - Structured Streaming! üåä
