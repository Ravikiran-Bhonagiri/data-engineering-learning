# Day 9: Production Deployment & Cluster Management

**Time:** 1-2 hours  
**Goal:** Deploy PySpark applications to production clusters.

---

## üöÄ spark-submit

The standard way to run Spark applications on a cluster:

```bash
spark-submit \
    --master yarn \
    --deploy-mode cluster \
    --executor-memory 4G \
    --num-executors 10 \
    --executor-cores 2 \
    my_script.py
```

### Key Options

| Option | Description | Example |
|--------|-------------|---------|
| `--master` | Cluster URL | `yarn`, `spark://host:7077`, `local[*]` |
| `--deploy-mode` | Driver location | `cluster` or `client` |
| `--executor-memory` | Memory per executor | `4G`, `8G` |
| `--num-executors` | Number of executors | `10`, `50` |
| `--executor-cores` | Cores per executor | `2`, `4` |
| `--driver-memory` | Driver memory | `2G` |
| `--conf` | Spark configs | `spark.sql.shuffle.partitions=200` |
| `--packages` | Maven packages | `io.delta:delta-core_2.12:2.4.0` |

---

## üéØ Deploy Modes

### Client Mode
- Driver runs on the machine you submit from
- Good for debugging, interactive work
- Logs visible locally

```bash
spark-submit --deploy-mode client my_script.py
```

### Cluster Mode
- Driver runs on a cluster node
- Good for production (resilient)
- Must check cluster logs

```bash
spark-submit --deploy-mode cluster my_script.py
```

---

## üì¶ Packaging Applications

### Package with dependencies

```bash
# Create requirements.txt
pip freeze > requirements.txt

# Package with pip
pip install --target=deps -r requirements.txt
zip -r deps.zip deps/

# Submit with packages
spark-submit --py-files deps.zip my_script.py
```

### Single file submission

```bash
# Simple script
spark-submit my_script.py arg1 arg2
```

---

## ‚òÅÔ∏è Cloud Deployments

### AWS EMR

```bash
# Submit to EMR
aws emr add-steps --cluster-id j-XXXXX --steps '[{
    "Name": "My Spark Job",
    "Type": "Spark",
    "Args": ["--deploy-mode", "cluster", "s3://bucket/script.py"]
}]'
```

### Databricks

```python
# In Databricks notebook - just run cells!
df = spark.read.csv("/data/sales.csv")
df.show()

# Or use Jobs API
dbutils.notebook.run("./my_notebook", 600, {"param": "value"})
```

### Google Dataproc

```bash
gcloud dataproc jobs submit pyspark \
    --cluster=my-cluster \
    --region=us-central1 \
    gs://bucket/my_script.py
```

---

## ‚öôÔ∏è Configuration Best Practices

### Memory Configuration

```bash
spark-submit \
    --executor-memory 4G \
    --driver-memory 2G \
    --conf spark.memory.fraction=0.8 \
    --conf spark.memory.storageFraction=0.5 \
    my_script.py
```

### Parallelism

```bash
spark-submit \
    --conf spark.sql.shuffle.partitions=200 \
    --conf spark.default.parallelism=100 \
    my_script.py
```

### Dynamic Allocation

```bash
spark-submit \
    --conf spark.dynamicAllocation.enabled=true \
    --conf spark.dynamicAllocation.minExecutors=5 \
    --conf spark.dynamicAllocation.maxExecutors=50 \
    my_script.py
```

---

## üìä Monitoring & Logging

### Spark UI
- Available at `http://driver-host:4040` during execution
- History Server for completed jobs

### Logging in Code

```python
import logging

# Get Spark logger
log4jLogger = spark._jvm.org.apache.log4j
logger = log4jLogger.LogManager.getLogger(__name__)

logger.info("Starting ETL job")
logger.warn("Large shuffle detected")
logger.error("Data validation failed")
```

### Access Logs

```bash
# YARN logs
yarn logs -applicationId application_1234567890_0001

# View in real-time
yarn logs -applicationId <app_id> -f
```

---

## üîÑ Job Scheduling

### Airflow Integration

```python
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

spark_task = SparkSubmitOperator(
    task_id='run_etl',
    application='/path/to/script.py',
    conn_id='spark_default',
    executor_memory='4G',
    num_executors=10,
)
```

### Cron (Simple)

```bash
# Run daily at 2 AM
0 2 * * * /opt/spark/bin/spark-submit /jobs/daily_etl.py
```

---

## üõ°Ô∏è Error Handling

```python
from pyspark.sql import SparkSession

def main():
    spark = SparkSession.builder.appName("robust_job").getOrCreate()
    
    try:
        # Your ETL logic
        df = spark.read.parquet("/data/input")
        result = df.groupBy("category").count()
        result.write.mode("overwrite").parquet("/data/output")
        
        # Log success
        print("Job completed successfully")
        
    except Exception as e:
        # Log failure
        print(f"Job failed: {str(e)}")
        raise
        
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
```

---

## üìã Production Checklist

- [ ] Use `cluster` deploy mode for production
- [ ] Set appropriate memory/cores based on data size
- [ ] Enable dynamic allocation for variable workloads
- [ ] Write logs to persistent storage (S3, HDFS)
- [ ] Set up alerting for failed jobs
- [ ] Use checkpointing for streaming jobs
- [ ] Test with subset of data first
- [ ] Monitor Spark UI for performance issues

---

## üß™ Practice Exercise

```python
# Create a production-ready ETL script:
# 1. Accept command-line arguments (input_path, output_path)
# 2. Add proper logging
# 3. Include error handling
# 4. Write to Delta table
# 5. Test with spark-submit

import sys
from pyspark.sql import SparkSession

def main(input_path, output_path):
    # TODO: Implement production ETL
    pass

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: script.py <input_path> <output_path>")
        sys.exit(1)
    main(sys.argv[1], sys.argv[2])
```

---

## üéØ Key Takeaways

‚úÖ **spark-submit:** Standard deployment method  
‚úÖ **Cluster mode:** Production deployments  
‚úÖ **Client mode:** Debugging/development  
‚úÖ **Dynamic allocation:** Scale executors automatically  
‚úÖ **Logging:** Use Spark's log4j integration  
‚úÖ **Airflow:** Production job scheduling  

---

**Congratulations!** üéâ You've completed the 10-day PySpark course!

**Next:** Check `03_QUICK_Reference.md` for a handy cheat sheet!
