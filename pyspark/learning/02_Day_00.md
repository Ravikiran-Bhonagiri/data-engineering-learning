# Day 0: What is PySpark & Why It Matters

**Time:** 30-45 minutes  
**Goal:** Understand PySpark's value through a real comparison with pandas.

---

## üîç Before PySpark: The Pandas Limitation

Imagine analyzing a year of web server logs (500 GB):

```python
import pandas as pd

# This WILL crash on a typical laptop
logs = pd.read_csv("web_logs_2025.csv")  # 500 GB file
# MemoryError!
```

**Pain points:**
1. **Memory crash:** Data must fit in RAM
2. **Slow processing:** Single-threaded
3. **No recovery:** If it crashes, start over

---

## ‚úÖ With PySpark: Same Analysis, Any Scale

```python
from pyspark.sql import SparkSession
import pyspark.sql.functions as F

# Create Spark session
spark = SparkSession.builder \
    .appName("web_log_analysis") \
    .master("local[*]") \
    .getOrCreate()

# Read distributed (works with 500 GB or 500 TB!)
logs = spark.read.csv("s3://data/web_logs_2025/", header=True, inferSchema=True)

# Compute daily metrics across entire cluster
metrics = logs.groupBy("date").agg(
    F.countDistinct("user_id").alias("unique_users"),
    F.count("url").alias("page_views"),
    F.avg("response_time").alias("avg_response_ms")
)

metrics.show()
```

**Output:**
```
+----------+------------+----------+---------------+
|      date|unique_users|page_views|avg_response_ms|
+----------+------------+----------+---------------+
|2025-01-01|     1234567|  45678901|          123.4|
|2025-01-02|     1345678|  48765432|          118.7|
+----------+------------+----------+---------------+
```

---

## üìä Pandas vs PySpark: Side-by-Side

```python
# Sample data (works in both)
data = [
    {"date": "2025-01-01", "user_id": 1, "url": "/home"},
    {"date": "2025-01-01", "user_id": 2, "url": "/about"},
    {"date": "2025-01-02", "user_id": 1, "url": "/contact"},
]

# ===== PANDAS =====
import pandas as pd

pdf = pd.DataFrame(data)
pandas_result = pdf.groupby("date").agg(
    unique_users=("user_id", pd.Series.nunique),
    page_views=("url", "count")
).reset_index()
print(pandas_result)

# ===== PYSPARK =====
from pyspark.sql import SparkSession
import pyspark.sql.functions as F

spark = SparkSession.builder.master("local[*]").appName("compare").getOrCreate()
sdf = spark.createDataFrame(data)
pyspark_result = sdf.groupBy("date").agg(
    F.countDistinct("user_id").alias("unique_users"),
    F.count("url").alias("page_views")
)
pyspark_result.show()
```

**Key differences:**

| Aspect | Pandas | PySpark |
|--------|--------|---------|
| **Memory** | Must fit in RAM | Distributed across cluster |
| **Parallelism** | Single-threaded | Multi-core, multi-node |
| **Data size** | < 10 GB typical | Unlimited (TB/PB) |
| **Speed** | Fast for small data | Faster for big data |
| **API** | Imperative | Lazy evaluation |

---

## üéØ When Does PySpark Make Sense?

### Use PySpark When:
- Data > 10 GB (or growing)
- Need to process daily/hourly
- Multiple data sources (S3, HDFS, databases)
- Production pipelines
- ML on large datasets

### Keep Using Pandas When:
- Data < 10 GB
- Quick EDA/prototyping
- Interactive notebooks
- Simple transformations

---

## üß™ Try It Yourself

**Exercise:** Run this comparison:

```python
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
import time

spark = SparkSession.builder.master("local[*]").appName("speed_test").getOrCreate()

# Create 1 million rows
df = spark.range(0, 1_000_000).withColumn("value", F.rand())

# Time an aggregation
start = time.time()
result = df.groupBy((F.col("id") % 100).alias("group")).agg(F.sum("value"))
result.count()  # Trigger execution
print(f"PySpark took: {time.time() - start:.2f} seconds")
```

---

## üéØ Key Takeaways

‚úÖ **Pandas:** Great for small data, single machine  
‚úÖ **PySpark:** Essential for big data, distributed processing  
‚úÖ **The same Python syntax concepts apply** (groupBy, agg, filter)  
‚úÖ **PySpark is lazy:** Nothing runs until you call an action  

---

**Next:** `02_Day_01.md` - Install PySpark and create your first Spark session! üîß
