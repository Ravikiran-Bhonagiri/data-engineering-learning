# Day 3: DataFrames Deep Dive

**Time:** 1-2 hours  
**Goal:** Master DataFrame creation, schema management, and basic operations.

---

## ğŸ“Š What is a DataFrame?

A **DataFrame** is a distributed collection of data organized into named columns (like a SQL table or pandas DataFrame).

**Why DataFrames over RDDs?**
- âœ… Schema enforcement
- âœ… Catalyst query optimization
- âœ… Tungsten memory management
- âœ… SQL-like syntax
- âœ… 10-100x faster than RDDs

---

## ğŸ›  Creating DataFrames

### From Python List

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

spark = SparkSession.builder.appName("dataframes").getOrCreate()

# Method 1: Infer schema
data = [("Alice", 30), ("Bob", 25), ("Charlie", 35)]
df = spark.createDataFrame(data, ["name", "age"])

# Method 2: Explicit schema
schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True)
])
df = spark.createDataFrame(data, schema)

df.show()
df.printSchema()
```

### From CSV/JSON/Parquet

```python
# CSV
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# JSON
df = spark.read.json("data.json")

# Parquet (recommended for big data)
df = spark.read.parquet("data.parquet")

# With options
df = spark.read \
    .option("header", True) \
    .option("inferSchema", True) \
    .option("delimiter", ";") \
    .csv("data.csv")
```

---

## ğŸ” Exploring DataFrames

```python
# Show data
df.show()          # First 20 rows
df.show(5)         # First 5 rows
df.show(5, False)  # Don't truncate columns

# Schema
df.printSchema()
df.dtypes          # List of (column, type)
df.columns         # Column names

# Statistics
df.describe().show()  # Count, mean, stddev, min, max
df.count()            # Row count
```

---

## ğŸ”§ Basic Operations

### Selecting Columns

```python
import pyspark.sql.functions as F

# Select columns
df.select("name", "age").show()
df.select(df.name, df.age).show()
df.select(F.col("name"), F.col("age")).show()

# Select with expression
df.select(F.col("name"), (F.col("age") + 10).alias("age_plus_10")).show()
```

### Filtering Rows

```python
# Filter
df.filter(df.age > 25).show()
df.filter(F.col("age") > 25).show()
df.where("age > 25").show()

# Multiple conditions
df.filter((df.age > 25) & (df.name != "Bob")).show()
df.filter((df.age > 25) | (df.name == "Bob")).show()
```

### Adding/Modifying Columns

```python
# Add new column
df = df.withColumn("age_doubled", F.col("age") * 2)

# Rename column
df = df.withColumnRenamed("name", "full_name")

# Drop column
df = df.drop("age_doubled")

# Cast type
df = df.withColumn("age", F.col("age").cast("string"))
```

---

## ğŸ“Š Aggregations

```python
from pyspark.sql.functions import sum, avg, count, max, min

employees = spark.createDataFrame([
    ("Alice", "Engineering", 120000),
    ("Bob", "Sales", 80000),
    ("Charlie", "Engineering", 95000),
    ("Diana", "HR", 70000),
], ["name", "dept", "salary"])

# Basic aggregations
employees.select(
    avg("salary").alias("avg_salary"),
    max("salary").alias("max_salary"),
    count("*").alias("total")
).show()

# Group by
employees.groupBy("dept").agg(
    avg("salary").alias("avg_salary"),
    count("*").alias("count")
).show()
```

---

## ğŸ”— Joins

```python
# Create second DataFrame
departments = spark.createDataFrame([
    ("Engineering", "Building A"),
    ("Sales", "Building B"),
    ("HR", "Building C"),
], ["dept", "location"])

# Inner join (default)
joined = employees.join(departments, "dept")

# Left join
joined = employees.join(departments, "dept", "left")

# Other join types
joined = employees.join(departments, "dept", "right")
joined = employees.join(departments, "dept", "outer")
joined = employees.join(departments, "dept", "cross")

joined.show()
```

---

## ğŸ“ withColumn Examples

```python
from pyspark.sql.functions import when, lit, concat, upper

df = spark.createDataFrame([
    ("Alice", 30, 120000),
    ("Bob", 25, 80000),
], ["name", "age", "salary"])

# Conditional logic
df = df.withColumn("seniority", 
    when(F.col("age") > 30, "Senior")
    .when(F.col("age") > 25, "Mid")
    .otherwise("Junior")
)

# String operations
df = df.withColumn("name_upper", upper(F.col("name")))
df = df.withColumn("greeting", concat(lit("Hello, "), F.col("name")))

# Add constant
df = df.withColumn("company", lit("Acme Corp"))

df.show()
```

---

## ğŸ§ª Practice Exercise

```python
# Create employee DataFrame
employees = spark.createDataFrame([
    (1, "Alice", "Engineering", 120000, "2020-01-15"),
    (2, "Bob", "Sales", 80000, "2019-06-01"),
    (3, "Charlie", "Engineering", 95000, "2021-03-20"),
    (4, "Diana", "HR", 70000, "2018-11-10"),
    (5, "Eve", "Sales", 85000, "2022-02-28"),
], ["id", "name", "dept", "salary", "hire_date"])

# TODO:
# 1. Add a column "bonus" = salary * 0.10
# 2. Filter employees hired after 2020
# 3. Group by department, get avg salary
# 4. Sort by avg salary descending
```

---

## ğŸ¯ Key Takeaways

âœ… **DataFrames > RDDs:** Faster, optimized, easier to use  
âœ… **Schema:** Enforced types prevent errors  
âœ… **`withColumn`:** Add/modify columns  
âœ… **`filter`/`where`:** Filter rows  
âœ… **`groupBy().agg()`:** Aggregations  
âœ… **`join`:** Combine DataFrames  

---

**Next:** `02_Day_04.md` - Spark SQL! ğŸ—ƒï¸
