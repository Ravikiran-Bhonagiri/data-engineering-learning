# Day 1: Installation & First Steps

**Time:** 1-2 hours  
**Goal:** Install PySpark and create your first Spark session.

---

## ðŸ›  Installation Options

### Option 1: pip (Recommended for Learning)

```bash
# Create virtual environment
python -m venv pyspark_env
source pyspark_env/bin/activate  # Windows: pyspark_env\Scripts\activate

# Install PySpark
pip install pyspark

# Verify installation
python -c "from pyspark.sql import SparkSession; print('PySpark installed!')"
```

**Requirements:**
- Python 3.8+
- Java 8 or 11 (required by Spark)

### Option 2: Conda

```bash
conda create -n pyspark python=3.10
conda activate pyspark
conda install -c conda-forge pyspark
```

### Option 3: Docker (No Java Setup!)

```bash
docker run -it --rm -p 8888:8888 jupyter/pyspark-notebook
```

---

## â˜• Java Setup (Required)

PySpark runs on the JVM, so Java is required.

**Windows:**
1. Download Java 11 from [Adoptium](https://adoptium.net/)
2. Install and set `JAVA_HOME`:
```powershell
# PowerShell
[Environment]::SetEnvironmentVariable("JAVA_HOME", "C:\Program Files\Eclipse Adoptium\jdk-11", "User")
```

**Mac:**
```bash
brew install openjdk@11
export JAVA_HOME=/usr/local/opt/openjdk@11
```

**Linux:**
```bash
sudo apt install openjdk-11-jdk
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

Verify:
```bash
java -version
# Should show: openjdk version "11.x.x"
```

---

## ðŸš€ Your First Spark Session

### Create a SparkSession

```python
from pyspark.sql import SparkSession

# Create Spark session (entry point to all PySpark functionality)
spark = SparkSession.builder \
    .appName("my_first_app") \
    .master("local[*]") \
    .getOrCreate()

# Check Spark version
print(f"Spark version: {spark.version}")

# Access Spark UI
print(f"Spark UI: http://localhost:4040")
```

**Key parameters:**
- `appName`: Name shown in Spark UI
- `master`: Cluster URL (`local[*]` = use all CPU cores locally)

---

## ðŸ“Š Create Your First DataFrame

### From Python List

```python
data = [
    ("Alice", 30, "Engineering"),
    ("Bob", 25, "Sales"),
    ("Charlie", 35, "Engineering"),
]
columns = ["name", "age", "department"]

df = spark.createDataFrame(data, columns)
df.show()
```

**Output:**
```
+-------+---+-----------+
|   name|age| department|
+-------+---+-----------+
|  Alice| 30|Engineering|
|    Bob| 25|      Sales|
|Charlie| 35|Engineering|
+-------+---+-----------+
```

### From Python Dict (pandas-like)

```python
data = [
    {"name": "Alice", "age": 30, "department": "Engineering"},
    {"name": "Bob", "age": 25, "department": "Sales"},
]
df = spark.createDataFrame(data)
df.show()
```

---

## ðŸ”§ Basic RDD Operations

RDDs are Spark's low-level API (foundational knowledge):

```python
# Create RDD from Python list
data = [("apple", 1), ("banana", 2), ("orange", 3), ("apple", 4)]
rdd = spark.sparkContext.parallelize(data)

# Transformation: double the numbers
doubled = rdd.map(lambda x: (x[0], x[1] * 2))

# Action: collect results
print(doubled.collect())
# [('apple', 2), ('banana', 4), ('orange', 6), ('apple', 8)]

# Reduce by key
totals = rdd.reduceByKey(lambda a, b: a + b)
print(totals.collect())
# [('apple', 5), ('banana', 2), ('orange', 3)]
```

---

## ðŸ“Š Spark UI

When running Spark, access the UI at `http://localhost:4040`:

| Tab | Shows |
|-----|-------|
| **Jobs** | All executed jobs |
| **Stages** | Task breakdown |
| **Storage** | Cached data |
| **Environment** | Configuration |
| **Executors** | Resource usage |
| **SQL** | Query plans |

---

## ðŸ§ª Practice Exercise

Create a script that:
1. Creates a SparkSession
2. Creates a DataFrame with employee data
3. Shows the data and schema

```python
from pyspark.sql import SparkSession

# 1. Create session
spark = SparkSession.builder \
    .appName("employee_analysis") \
    .master("local[*]") \
    .getOrCreate()

# 2. Create sample data
employees = [
    (1, "Alice", "Engineering", 120000),
    (2, "Bob", "Sales", 80000),
    (3, "Charlie", "Engineering", 95000),
    (4, "Diana", "HR", 70000),
    (5, "Eve", "Sales", 85000),
]

df = spark.createDataFrame(employees, ["id", "name", "dept", "salary"])

# 3. Explore data
df.show()
df.printSchema()
print(f"Total employees: {df.count()}")

# Stop session when done
spark.stop()
```

---

## ðŸŽ¯ Key Takeaways

âœ… **SparkSession** is the entry point to PySpark  
âœ… **Java required:** Install Java 8 or 11  
âœ… **`local[*]`** uses all CPU cores locally  
âœ… **Spark UI** at `localhost:4040` for monitoring  
âœ… **RDDs** are low-level; **DataFrames** are preferred  

---

**Next:** `02_Day_02.md` - Master RDD transformations and actions! ðŸ”„
