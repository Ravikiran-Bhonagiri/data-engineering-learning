# Day 2: Mastering RDDs

**Time:** 1-2 hours  
**Goal:** Understand RDD transformations, actions, and fault tolerance.

---

## üìö What is an RDD?

**RDD = Resilient Distributed Dataset**

- **Resilient:** Fault-tolerant via lineage
- **Distributed:** Partitioned across cluster nodes
- **Dataset:** Collection of elements

RDDs are the foundational abstraction in Spark (DataFrames are built on top of RDDs).

---

## üîÑ Transformations vs Actions

| Type | Description | Examples | Lazy? |
|------|-------------|----------|-------|
| **Transformations** | Create new RDD | `map`, `filter`, `flatMap` | ‚úÖ Yes |
| **Actions** | Return result | `collect`, `count`, `save` | ‚ùå No |

```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])

# Transformation (lazy - nothing happens yet!)
doubled = rdd.map(lambda x: x * 2)

# Action (triggers execution!)
result = doubled.collect()  # [2, 4, 6, 8, 10]
```

---

## üîß Common RDD Transformations

### map() - Transform each element

```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4])
squared = rdd.map(lambda x: x ** 2)
print(squared.collect())  # [1, 4, 9, 16]
```

### filter() - Keep elements matching condition

```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6])
evens = rdd.filter(lambda x: x % 2 == 0)
print(evens.collect())  # [2, 4, 6]
```

### flatMap() - One-to-many transformation

```python
lines = spark.sparkContext.parallelize(["hello world", "foo bar"])
words = lines.flatMap(lambda line: line.split(" "))
print(words.collect())  # ['hello', 'world', 'foo', 'bar']
```

### reduceByKey() - Aggregate by key

```python
pairs = spark.sparkContext.parallelize([
    ("apple", 1), ("banana", 2), ("apple", 3), ("banana", 1)
])
totals = pairs.reduceByKey(lambda a, b: a + b)
print(totals.collect())  # [('apple', 4), ('banana', 3)]
```

### groupByKey() - Group values by key

```python
pairs = spark.sparkContext.parallelize([
    ("fruit", "apple"), ("fruit", "banana"), ("vegetable", "carrot")
])
grouped = pairs.groupByKey()
result = grouped.mapValues(list).collect()
# [('fruit', ['apple', 'banana']), ('vegetable', ['carrot'])]
```

---

## ‚ö° Common RDD Actions

| Action | Description |
|--------|-------------|
| `collect()` | Return all elements to driver |
| `count()` | Count elements |
| `take(n)` | Return first n elements |
| `first()` | Return first element |
| `reduce(func)` | Aggregate with function |
| `saveAsTextFile(path)` | Save to file |

```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])

print(rdd.count())      # 5
print(rdd.first())      # 1
print(rdd.take(3))      # [1, 2, 3]
print(rdd.reduce(lambda a, b: a + b))  # 15 (sum)
```

---

## üìù Word Count Example

Classic big data example:

```python
# Read text file
text_rdd = spark.sparkContext.textFile("book.txt")

# Word count pipeline
word_counts = (
    text_rdd
    .flatMap(lambda line: line.split(" "))         # Split into words
    .map(lambda word: word.lower().strip())        # Normalize
    .filter(lambda word: len(word) > 0)            # Remove empty
    .map(lambda word: (word, 1))                   # Create pairs
    .reduceByKey(lambda a, b: a + b)               # Sum counts
    .sortBy(lambda x: -x[1])                       # Sort descending
)

# Top 10 words
for word, count in word_counts.take(10):
    print(f"{word}: {count}")
```

---

## üîí Fault Tolerance: Lineage

RDDs track their **lineage** (how they were created):

```python
rdd1 = spark.sparkContext.parallelize([1, 2, 3, 4])
rdd2 = rdd1.map(lambda x: x * 2)
rdd3 = rdd2.filter(lambda x: x > 4)

# View lineage
print(rdd3.toDebugString())
```

**Output:**
```
(2) PythonRDD[3] at RDD at PythonRDD.scala:53
 |  PythonRDD[2] at RDD at PythonRDD.scala:53
 |  ParallelCollectionRDD[1] at parallelize at ...
```

If a partition is lost, Spark recomputes it using lineage!

---

## üíæ Caching RDDs

Avoid recomputation for frequently used RDDs:

```python
# Without caching (recomputes every time)
rdd = spark.sparkContext.textFile("large_file.txt")
filtered = rdd.filter(lambda x: "error" in x)
print(filtered.count())  # Reads file
print(filtered.take(10)) # Reads file AGAIN!

# With caching
cached = rdd.filter(lambda x: "error" in x).cache()
print(cached.count())  # Reads file, stores in memory
print(cached.take(10)) # Uses cached data!
```

Storage levels:
```python
from pyspark import StorageLevel

rdd.persist(StorageLevel.MEMORY_ONLY)      # Default
rdd.persist(StorageLevel.MEMORY_AND_DISK)  # Spill to disk
rdd.persist(StorageLevel.DISK_ONLY)        # Disk only
```

---

## ‚ö†Ô∏è RDD Limitations

| Issue | Problem |
|-------|---------|
| **No optimization** | No Catalyst query optimizer |
| **Verbose code** | More boilerplate needed |
| **No schema** | Data types not enforced |
| **Slower** | No Tungsten memory optimization |

**Recommendation:** Use DataFrames for most work. RDDs for:
- Custom serialization
- Low-level control
- Unstructured data

---

## üß™ Practice Exercise

Create a word count program:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]").appName("wordcount").getOrCreate()

# Sample text
text = """
Apache Spark is a unified analytics engine for big data processing
Spark provides high-level APIs in Java Scala Python and R
Spark also supports SQL and streaming and machine learning
"""

# Create RDD from text
lines = spark.sparkContext.parallelize(text.strip().split("\n"))

# TODO: Implement word count
# 1. Split lines into words
# 2. Convert to lowercase
# 3. Create (word, 1) pairs
# 4. Reduce by key
# 5. Print top 5 words

spark.stop()
```

---

## üéØ Key Takeaways

‚úÖ **RDDs:** Resilient Distributed Datasets  
‚úÖ **Transformations are lazy:** Nothing runs until action  
‚úÖ **Actions trigger execution:** `collect`, `count`, `save`  
‚úÖ **Lineage provides fault tolerance**  
‚úÖ **Cache frequently used RDDs**  
‚úÖ **Use DataFrames for most work** (better optimization)  

---

**Next:** `02_Day_03.md` - DataFrames deep dive! üìä
