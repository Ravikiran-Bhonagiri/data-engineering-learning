# Day 8: Delta Lake & Data Lakehouse

**Time:** 1-2 hours  
**Goal:** Learn modern data lakehouse patterns with Delta Lake.

---

## ğŸ  What is Delta Lake?

**Delta Lake** is an open-source storage layer that brings:
- **ACID transactions** to data lakes
- **Schema enforcement** and evolution
- **Time travel** (query previous versions)
- **Unified batch + streaming**

```
Traditional Data Lake     â†’     Delta Lake
â”œâ”€â”€ No transactions       â”œâ”€â”€ ACID transactions
â”œâ”€â”€ No schema             â”œâ”€â”€ Schema enforcement
â”œâ”€â”€ Overwrite only        â”œâ”€â”€ Upserts (MERGE)
â””â”€â”€ No versioning         â””â”€â”€ Time travel
```

---

## ğŸ›  Setup Delta Lake

### Local Setup

```python
# Install
pip install delta-spark

# Create SparkSession with Delta
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("delta_lake") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.4.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()
```

---

## ğŸ“ Basic Delta Operations

### Write Delta Table

```python
data = spark.createDataFrame([
    (1, "Alice", 30),
    (2, "Bob", 25),
    (3, "Charlie", 35),
], ["id", "name", "age"])

# Write as Delta
data.write.format("delta").save("/data/people")

# Or with partitioning
data.write.format("delta").partitionBy("age").save("/data/people_partitioned")
```

### Read Delta Table

```python
# Read
df = spark.read.format("delta").load("/data/people")
df.show()

# As table
spark.sql("CREATE TABLE people USING DELTA LOCATION '/data/people'")
spark.sql("SELECT * FROM people").show()
```

---

## ğŸ”„ MERGE (Upsert)

The killer feature: Update existing rows, insert new ones!

```python
from delta.tables import DeltaTable

# Existing data
delta_table = DeltaTable.forPath(spark, "/data/people")

# New/updated data
updates = spark.createDataFrame([
    (1, "Alice", 31),    # Update Alice's age
    (4, "Diana", 28),    # New person
], ["id", "name", "age"])

# Merge (upsert)
delta_table.alias("target").merge(
    updates.alias("source"),
    "target.id = source.id"
).whenMatchedUpdateAll() \
 .whenNotMatchedInsertAll() \
 .execute()

# Result
spark.read.format("delta").load("/data/people").show()
```

---

## â° Time Travel

Query previous versions of your data:

```python
# Read specific version
df_v0 = spark.read.format("delta").option("versionAsOf", 0).load("/data/people")

# Read at specific timestamp
df_old = spark.read.format("delta") \
    .option("timestampAsOf", "2025-01-01 00:00:00") \
    .load("/data/people")

# View history
from delta.tables import DeltaTable
delta_table = DeltaTable.forPath(spark, "/data/people")
delta_table.history().show()
```

---

## ğŸ”§ Schema Evolution

```python
# Enable auto-merge for new columns
spark.sql("SET spark.databricks.delta.schema.autoMerge.enabled = true")

# Add new data with extra column
new_data = spark.createDataFrame([
    (5, "Eve", 32, "Engineering"),  # New "dept" column
], ["id", "name", "age", "dept"])

new_data.write.format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .save("/data/people")
```

---

## ğŸ§¹ Maintenance Operations

### Optimize (Compact Files)

```python
from delta.tables import DeltaTable

delta_table = DeltaTable.forPath(spark, "/data/people")

# Compact small files
delta_table.optimize().executeCompaction()

# With Z-Order for faster queries
delta_table.optimize().executeZOrderBy("id")
```

### Vacuum (Clean Old Files)

```python
# Remove files older than 7 days
delta_table.vacuum(retentionHours=168)
```

---

## ğŸ“Š Streaming with Delta

```python
# Write streaming data to Delta
stream_df.writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "/checkpoints/stream") \
    .start("/data/events")

# Read as stream
streaming_df = spark.readStream \
    .format("delta") \
    .load("/data/events")
```

---

## ğŸ—ï¸ Medallion Architecture

Best practice for data lakehouse:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BRONZE    â”‚ â”€â”€â–º â”‚   SILVER    â”‚ â”€â”€â–º â”‚    GOLD     â”‚
â”‚  (Raw Data) â”‚     â”‚  (Cleaned)  â”‚     â”‚ (Aggregated)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
# Bronze: Raw ingestion
raw_df.write.format("delta").save("/data/bronze/sales")

# Silver: Clean and dedupe
bronze = spark.read.format("delta").load("/data/bronze/sales")
silver = bronze.dropDuplicates(["transaction_id"]).dropna()
silver.write.format("delta").save("/data/silver/sales")

# Gold: Business aggregates
silver = spark.read.format("delta").load("/data/silver/sales")
gold = silver.groupBy("product_category").agg(sum("amount").alias("total_sales"))
gold.write.format("delta").save("/data/gold/sales_by_category")
```

---

## ğŸ§ª Practice Exercise

```python
# Build a Delta Lake pipeline:
# 1. Create Delta table with customer data
# 2. Perform MERGE to upsert new customers
# 3. Use time travel to compare versions
# 4. Optimize and vacuum the table
```

---

## ğŸ¯ Key Takeaways

âœ… **Delta Lake:** ACID transactions on data lake  
âœ… **MERGE:** Upsert (update + insert)  
âœ… **Time travel:** Query historical versions  
âœ… **Schema evolution:** Add columns safely  
âœ… **Medallion architecture:** Bronze â†’ Silver â†’ Gold  
âœ… **Optimize + Vacuum:** Maintenance operations  

---

**Next:** `02_Day_09.md` - Production Deployment! ğŸš€
