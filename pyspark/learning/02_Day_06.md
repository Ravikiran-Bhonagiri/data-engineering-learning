# Day 6: MLlib Basics

**Time:** 1-2 hours  
**Goal:** Learn machine learning fundamentals with Spark MLlib.

---

## ü§ñ What is MLlib?

MLlib is Spark's machine learning library:
- **Scalable:** Train on billions of rows
- **Distributed:** Uses cluster for training
- **Integrated:** Works with DataFrames

---

## üì¶ MLlib Components

| Component | Purpose | Examples |
|-----------|---------|----------|
| **Transformers** | Transform data | VectorAssembler, StandardScaler |
| **Estimators** | Train models | LogisticRegression, RandomForest |
| **Pipelines** | Chain operations | End-to-end workflows |
| **Evaluators** | Measure performance | BinaryClassificationEvaluator |

---

## üéØ Classification Example

### Prepare Data

```python
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator

spark = SparkSession.builder.appName("ml_basics").getOrCreate()

# Sample data
data = spark.createDataFrame([
    (0, 22.0, 7.25, 0),
    (1, 38.0, 71.28, 1),
    (1, 26.0, 7.92, 1),
    (0, 35.0, 53.10, 0),
    (0, 28.0, 8.05, 0),
    (1, 45.0, 83.47, 1),
], ["survived", "age", "fare", "pclass"])

data.show()
```

### Feature Engineering

```python
# Assemble features into vector
assembler = VectorAssembler(
    inputCols=["age", "fare", "pclass"],
    outputCol="features"
)
data = assembler.transform(data)
data.select("features", "survived").show()
```

**Output:**
```
+----------------+--------+
|        features|survived|
+----------------+--------+
|[22.0,7.25,0.0] |       0|
|[38.0,71.28,1.0]|       1|
+----------------+--------+
```

### Train Model

```python
# Split data
train, test = data.randomSplit([0.8, 0.2], seed=42)

# Train logistic regression
lr = LogisticRegression(
    featuresCol="features",
    labelCol="survived",
    maxIter=10
)
model = lr.fit(train)

# View coefficients
print(f"Coefficients: {model.coefficients}")
print(f"Intercept: {model.intercept}")
```

### Evaluate Model

```python
# Make predictions
predictions = model.transform(test)
predictions.select("survived", "prediction", "probability").show()

# Evaluate
evaluator = BinaryClassificationEvaluator(labelCol="survived")
auc = evaluator.evaluate(predictions)
print(f"AUC: {auc}")
```

---

## üîß Common Transformers

### VectorAssembler

```python
from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(
    inputCols=["age", "fare", "pclass"],
    outputCol="features"
)
df = assembler.transform(df)
```

### StandardScaler

```python
from pyspark.ml.feature import StandardScaler

scaler = StandardScaler(
    inputCol="features",
    outputCol="scaled_features",
    withStd=True,
    withMean=True
)
scaler_model = scaler.fit(df)
df = scaler_model.transform(df)
```

### StringIndexer (Encode categories)

```python
from pyspark.ml.feature import StringIndexer

indexer = StringIndexer(
    inputCol="gender",
    outputCol="gender_index"
)
df = indexer.fit(df).transform(df)
```

### OneHotEncoder

```python
from pyspark.ml.feature import OneHotEncoder

encoder = OneHotEncoder(
    inputCol="gender_index",
    outputCol="gender_vec"
)
df = encoder.fit(df).transform(df)
```

---

## üìä Common Algorithms

### Classification

```python
from pyspark.ml.classification import (
    LogisticRegression,
    RandomForestClassifier,
    GBTClassifier,
    DecisionTreeClassifier
)

# Logistic Regression
lr = LogisticRegression(featuresCol="features", labelCol="label")

# Random Forest
rf = RandomForestClassifier(numTrees=100, maxDepth=5)

# Gradient Boosted Trees
gbt = GBTClassifier(maxIter=10)
```

### Regression

```python
from pyspark.ml.regression import (
    LinearRegression,
    RandomForestRegressor,
    GBTRegressor
)

# Linear Regression
lr = LinearRegression(featuresCol="features", labelCol="target")

# Random Forest Regressor
rf = RandomForestRegressor(numTrees=100)
```

### Clustering

```python
from pyspark.ml.clustering import KMeans

kmeans = KMeans(k=3, seed=42)
model = kmeans.fit(df)
predictions = model.transform(df)
```

---

## üìè Evaluation Metrics

### Classification

```python
from pyspark.ml.evaluation import (
    BinaryClassificationEvaluator,
    MulticlassClassificationEvaluator
)

# Binary (AUC)
evaluator = BinaryClassificationEvaluator(labelCol="label")
auc = evaluator.evaluate(predictions)

# Multiclass (Accuracy, F1)
evaluator = MulticlassClassificationEvaluator(
    labelCol="label",
    metricName="accuracy"  # or "f1", "precision", "recall"
)
accuracy = evaluator.evaluate(predictions)
```

### Regression

```python
from pyspark.ml.evaluation import RegressionEvaluator

evaluator = RegressionEvaluator(
    labelCol="target",
    metricName="rmse"  # or "mse", "mae", "r2"
)
rmse = evaluator.evaluate(predictions)
```

---

## üß™ Practice Exercise

```python
# Create sample data
data = spark.createDataFrame([
    (1.0, 2.0, 0),
    (2.0, 3.0, 0),
    (3.0, 4.0, 0),
    (10.0, 11.0, 1),
    (11.0, 12.0, 1),
    (12.0, 13.0, 1),
], ["feature1", "feature2", "label"])

# TODO:
# 1. Assemble features
# 2. Split into train/test
# 3. Train RandomForestClassifier
# 4. Evaluate with accuracy
```

---

## üéØ Key Takeaways

‚úÖ **VectorAssembler:** Combine features into vector  
‚úÖ **StringIndexer:** Encode categorical variables  
‚úÖ **fit() then transform():** Standard workflow  
‚úÖ **Evaluators:** Measure model performance  
‚úÖ **Same API:** Classification, regression, clustering  

---

**Next:** `02_Day_07.md` - ML Pipelines & Feature Engineering! üîß
