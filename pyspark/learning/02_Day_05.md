# Day 5: Structured Streaming

**Time:** 1-2 hours  
**Goal:** Build real-time data processing pipelines with Spark Streaming.

---

## üåä What is Structured Streaming?

Structured Streaming treats streaming data as an **unbounded table** that grows continuously. You write the same DataFrame/SQL code for both batch and streaming!

```
+------------------+
|  Unbounded Table |  ‚Üê New rows arrive continuously
+------------------+
|    Row 1         |
|    Row 2         |
|    Row 3         |
|    ...           |  ‚Üê Infinite rows
+------------------+
```

---

## üöÄ Your First Streaming Application

### Word Count from Socket

**Step 1: Start netcat server (Terminal 1):**
```bash
nc -lk 9999
```

**Step 2: Run streaming app (Terminal 2):**

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, split

spark = SparkSession.builder.appName("streaming_wordcount").getOrCreate()

# Read from socket
lines = spark.readStream \
    .format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()

# Split lines into words
words = lines.select(explode(split(lines.value, " ")).alias("word"))

# Count words
wordCounts = words.groupBy("word").count()

# Write to console
query = wordCounts.writeStream \
    .outputMode("complete") \
    .format("console") \
    .start()

query.awaitTermination()
```

**Step 3: Type words in netcat terminal and see counts update!**

---

## üì• Input Sources

### File Source (CSV, JSON, Parquet)

```python
# Watch a directory for new files
df = spark.readStream \
    .format("csv") \
    .option("header", True) \
    .schema(schema) \
    .load("/path/to/streaming/data/")
```

### Kafka Source

```python
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "my_topic") \
    .load()

# Parse Kafka value (usually JSON)
from pyspark.sql.functions import from_json, col

schema = "user_id INT, event STRING, timestamp TIMESTAMP"
parsed = df.select(
    from_json(col("value").cast("string"), schema).alias("data")
).select("data.*")
```

---

## üì§ Output Modes

| Mode | Description | Use Case |
|------|-------------|----------|
| **Append** | Only new rows | Simple transforms |
| **Complete** | Entire result table | Aggregations |
| **Update** | Only changed rows | Aggregations with changes |

```python
# Append mode (no aggregation)
query = df.writeStream \
    .outputMode("append") \
    .format("parquet") \
    .option("path", "/output") \
    .start()

# Complete mode (aggregation)
query = df.groupBy("category").count() \
    .writeStream \
    .outputMode("complete") \
    .format("console") \
    .start()
```

---

## üì§ Output Sinks

### Parquet Files

```python
query = df.writeStream \
    .format("parquet") \
    .option("path", "/data/output/") \
    .option("checkpointLocation", "/data/checkpoint/") \
    .start()
```

### Kafka

```python
query = df.writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("topic", "output_topic") \
    .start()
```

### Memory (for debugging)

```python
query = df.writeStream \
    .format("memory") \
    .queryName("my_stream") \
    .start()

# Query the stream
spark.sql("SELECT * FROM my_stream").show()
```

---

## ‚è∞ Windowed Aggregations

### Tumbling Window (Non-overlapping)

```python
from pyspark.sql.functions import window

# Events with timestamp
events = spark.readStream.format("rate").load()

# 5-minute tumbling windows
windowed = events.groupBy(
    window(col("timestamp"), "5 minutes")
).count()

windowed.writeStream \
    .outputMode("complete") \
    .format("console") \
    .start()
```

### Sliding Window (Overlapping)

```python
# 10-minute window, sliding every 5 minutes
windowed = events.groupBy(
    window(col("timestamp"), "10 minutes", "5 minutes")
).count()
```

---

## üíß Watermarking (Handle Late Data)

```python
from pyspark.sql.functions import window

# Allow data up to 10 minutes late
events \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(window("timestamp", "5 minutes")) \
    .count()
```

---

## ‚öôÔ∏è Triggers

```python
# Process as fast as possible (default)
query = df.writeStream.trigger(processingTime="0 seconds")

# Process every 10 seconds
query = df.writeStream.trigger(processingTime="10 seconds")

# Process once and stop
query = df.writeStream.trigger(once=True)
```

---

## üß™ Practice Exercise

```python
# Generate rate stream (1 row/second with timestamp and value)
events = spark.readStream \
    .format("rate") \
    .option("rowsPerSecond", 1) \
    .load()

# TODO:
# 1. Add a column "category" = value % 3
# 2. Group by category and count
# 3. Use 10-second tumbling windows
# 4. Output to console
```

---

## üéØ Key Takeaways

‚úÖ **Unbounded table:** Stream = infinite DataFrame  
‚úÖ **Same API:** Batch and streaming use same code  
‚úÖ **Output modes:** Append, Complete, Update  
‚úÖ **Checkpointing:** Required for fault tolerance  
‚úÖ **Watermarking:** Handle late data  
‚úÖ **Windows:** Time-based aggregations  

---

**Next:** `02_Day_06.md` - MLlib Basics! ü§ñ
