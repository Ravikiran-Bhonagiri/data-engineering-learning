# Daily Workflow & Patterns üìÖ

[‚Üê Back to Main](README.md)

How Azure services fit together in real-world data pipelines.

---

## Standard Azure Data Pipeline

### Phase 1: Ingestion

**Batch:**
- Files ‚Üí Blob Storage / Data Lake Gen2

**Streaming:**
- Events ‚Üí Event Hubs

### Phase 2: Processing

**Option A:** Data Factory (Visual ETL)
- Simple transformations
- Orchestration

**Option B:** Databricks (Complex Spark)
- ML pipelines
- Advanced transformations

**Option C:** Synapse Spark
- Integrated with Synapse workspace

### Phase 3: Storage

**Data Lake Structure:**
```
/raw/           # As-is data
/staging/       # Cleaned
/curated/       # Analytics-ready
```

**Load to Warehouse:**
- Synapse Dedicated SQL (production)
- Synapse Serverless SQL (exploration)

### Phase 4: Analytics

- Power BI ‚Üí Synapse/SQL Database
- Analysts ‚Üí Synapse Serverless
- Applications ‚Üí SQL Database

### Phase 5: Monitor

- Azure Monitor for alerts
- Data Factory monitoring
- Purview for lineage

---

## Sample End-to-End Pipeline

**Use Case:** Daily sales processing

```
1. Source System
   ‚Üì (nightly export)
2. BLOB STORAGE (raw/)
   ‚Üì (trigger)
3. DATA FACTORY Pipeline
   - Copy to staging
   - Transform with mapping data flow
   ‚Üì
4. DATA LAKE (curated/)
   ‚Üì (load)
5. SYNAPSE DEDICATED SQL
   ‚Üì
6. POWER BI Dashboards
```

**Costs (100GB/day):**
- Blob Storage: $1.84/month
- Data Factory: ~$30/month
- Synapse DW100c (8hrs/day): ~$192/month
- **Total: ~$224/month**

---

## Decision Tree

### "I need to query data..."

**Where is it?**
- Blob Storage ‚Üí Synapse Serverless or load to dedicated
- SQL Database ‚Üí Query directly or sync to Synapse

**How often?**
- Ad-hoc ‚Üí Synapse Serverless
- Daily heavy use ‚Üí Synapse Dedicated (with pause/resume)

### "I need to transform data..."

**Complexity?**
- Simple ‚Üí Data Factory mapping data flows
- Complex/ML ‚Üí Databricks

**Volume?**
- <1TB ‚Üí Data Factory
- >1TB ‚Üí Databricks or Synapse Spark

### "I need orchestration..."

**Steps?**
- <10 steps ‚Üí Data Factory
- Complex dependencies ‚Üí Consider Apache Airflow

---

## Best Practices

1. **Use Data Lake Gen2** for analytics workloads
2. **Partition** by date in Synapse
3. **Pause** dedicated SQL pools when not in use
4. **Start serverless** before committing to dedicated
5. **Monitor** from day 1 with Azure Monitor
6. **Tag resources** for cost tracking
7. **Use Purview** early for governance

[‚Üê Back to Main](README.md)
