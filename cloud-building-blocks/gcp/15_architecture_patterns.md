# Architecture Patterns ğŸ—ï¸

[â† Back to Main](README.md)

Common GCP data engineering architecture patterns and best practices.

---

## Pattern 1: Batch Analytics (Classic Data Lake)

```
Cloud Storage (Raw Data)
    â†“
Dataflow (Batch Processing)
    â†“
BigQuery (Data Warehouse)
    â†“
Looker/Data Studio (BI)
```

**When:** Daily/hourly data processing
**Cost:** Low (pay per job)
**Complexity:** Low
**Example:** Daily sales reports

---

## Pattern 2: Streaming Analytics (Real-Time)

```
Application
    â†“
Pub/Sub (Message Queue)
    â†“
Dataflow Streaming (Processing)
    â†“
BigQuery (Storage + Analytics)
    â†“
BI Engine (Sub-second queries)
```

**When:** <1 minute latency required
**Cost:** Medium (persistent workers)
**Complexity:** Medium
**Example:** Real-time user activity dashboard

---

## Pattern 3: Lambda Architecture (Batch + Streaming)

```
         Application
         â†™          â†˜
    Pub/Sub      Cloud Storage
    (Real-time)   (Batch)
         â†“            â†“
    Dataflow      Dataflow
    Streaming     Batch
         â†˜          â†™
         BigQuery
```

**When:** Need both real-time and batch processing
**Cost:** High (runs both pipelines)
**Complexity:** High
**Example:** E-commerce (real-time inventory + daily reports)

---

## Pattern 4: Database Replication (CDC)

```
Cloud SQL (OLTP)
    â†“
Datastream (CDC)
    â†“
BigQuery (OLAP)
    â†“
Analytics
```

**When:** Keep analytics in sync with operational DB
**Cost:** ~$0.04/GB processed
**Complexity:** Low
**Example:** Customer 360 view

---

## Pattern 5: Data Lake with Governance

```
Cloud Storage (Data Lake)
    â†“
Data Catalog (Discovery)
    â†“
DLP (PII Detection)
    â†“
BigQuery (with Policy Tags)
    â†“
IAM (Access Control)
```

**When:** Compliance requirements (GDPR, HIPAA)
**Cost:** DLP scan costs (~$0.15/GB one-time)
**Complexity:** Medium
**Example:** Healthcare data platform

---

## Pattern 6: ML Pipeline

```
BigQuery (Feature Store)
    â†“
Vertex AI (Train Model)
    â†“
Vertex Endpoints (Serve Predictions)
    â†“
BigQuery ML (or) API
```

**When:** Production ML workflows
**Cost:** Training + inference costs
**Complexity:** High
**Example:** Customer churn prediction

---

## Pattern 7: Multi-Cloud Data Sharing

```
BigQuery Omni
    â†“
Query AWS S3 / Azure Blob
    â†“
Results in BigQuery
```

**When:** Data in multiple clouds
**Cost:** Query costs + cross-cloud egress
**Complexity:** Medium
**Example:** Acquisitions, multi-cloud strategy

---

## Pattern 8: Event-Driven Automation

```
Cloud Storage Upload
    â†“
Cloud Functions (Trigger)
    â†“
Dataflow Job (Start)
    â†“
BigQuery (Load)
    â†“
Workflows (Notify)
```

**When:** Automated, event-driven pipelines
**Cost:** Low (serverless, pay-per-use)
**Complexity:** Low
**Example:** Process uploaded CSV files automatically

---

## Best Practices

### 1. Data Organization

**Cloud Storage:**
```
gs://bucket/
â”œâ”€â”€ raw/            # As-is data
â”œâ”€â”€ staging/        # Cleaned/validated
â”œâ”€â”€ curated/        # Analytics-ready
â””â”€â”€ archive/        # Old data (Archive tier)
```

**BigQuery:**
```
project/
â”œâ”€â”€ raw_data/       # Loaded from sources
â”œâ”€â”€ staging/        # Transformations
â”œâ”€â”€ analytics/      # Final models
â””â”€â”€ backups/        # Historical snapshots
```

### 2. Security Layers

1. **Network:** VPC, Private Google Access
2. **IAM:** Least privilege, service accounts
3. **Data:** Encryption at rest (default), DLP
4. **Audit:** Cloud Logging, Data Catalog

### 3. Cost Management

- **Start:** On-demand pricing
- **Scale:** Committed use discounts
- **Optimize:** Partitioning, lifecycle policies
- **Monitor:** Budget alerts, cost breakdowns

### 4. Monitoring Strategy

1. **Data Freshness:** Alert if data >30min old
2. **Pipeline Health:** Dataflow lag, job failures
3. **Cost Alerts:** Daily spend thresholds
4. **Data Quality:** Row counts, null checks

---

## Anti-Patterns to Avoid

âŒ **Storing everything in BigQuery**
â†’ Use Cloud Storage for raw files

âŒ **No partitioning on large tables**
â†’ Always partition by date/timestamp

âŒ **24/7 Dataproc clusters**
â†’ Use ephemeral clusters or Dataflow

âŒ **SELECT * in production queries**
â†’ Select only needed columns

âŒ **No data governance**
â†’ Implement IAM, DLP, Data Catalog early

âŒ **Over-engineering for small data**
â†’ Start simple (BigQuery SQL), add complexity as needed

---

## Recommended Architecture Evolution

**Phase 1: Start Simple (0-100GB)**
- Cloud Storage + BigQuery + BigQuery SQL
- Cost: ~$50/month

**Phase 2: Add Processing (100GB-1TB)**
- Add Dataflow for complex ETL
- Cost: ~$200/month

**Phase 3: Add Streaming (Real-time needs)**
- Add Pub/Sub + Dataflow Streaming
- Cost: ~$500/month

**Phase 4: Add Governance (Compliance)**
- Add Data Catalog + DLP + IAM policies
- Cost: +$100/month one-time DLP scans

**Phase 5: Scale & Optimize (>1TB)**
- Flat-rate slots, committed use, advanced tuning
- Cost: $2,000-5,000/month

---

[â† Back to Main](README.md)
