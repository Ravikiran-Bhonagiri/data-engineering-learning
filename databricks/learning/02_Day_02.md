# Day 2: Delta Lake Mastery

**Time:** 1-2 hours  
**Goal:** Deep dive into Delta Lake internals, optimization, and time travel.

---

## ðŸ“ What Makes Delta "Delta"?

Delta Lake adds a **Transaction Log** (`_delta_log`) to Parquet files.

**The Transaction Log:**
- Stored as JSON files in `_delta_log/`.
- Tracks **ACID transactions** (Atomicity, Consistency, Isolation, Durability).
- Contains metadata (schema, stats, file paths).

```
/data/my_table/
    â”œâ”€â”€ _delta_log/
    â”‚   â”œâ”€â”€ 000000.json
    â”‚   â”œâ”€â”€ 000001.json
    â”œâ”€â”€ part-00001.snappy.parquet
    â”œâ”€â”€ part-00002.snappy.parquet
```

---

## ðŸ›  Hands-on: Advanced Delta Features

### 1. Schema Evolution (`mergeSchema`)

```python
# Create table
df = spark.createDataFrame([(1, "a")], ["id", "val"])
df.write.format("delta").save("/tmp/delta_test")

# Add new column (This typically fails in Parquet)
df_new = spark.createDataFrame([(2, "b", "new")], ["id", "val", "extra"])

# Enable Schema Evolution
df_new.write \
    .format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .save("/tmp/delta_test")
```

### 2. Time Travel

Query the table state at a previous point in time.

```sql
-- SQL Syntax
SELECT * FROM my_table TIMESTAMP AS OF '2025-01-01 10:00:00';
SELECT * FROM my_table VERSION AS OF 5;
```

```python
# Python Syntax
df = spark.read.format("delta").option("versionAsOf", 5).load("/tmp/delta_test")
```

### 3. Restore Table

Mistake made? Roll back instantly.

```sql
RESTORE TABLE my_table TO VERSION AS OF 4;
```

### 4. Optimize & Z-Order (Performance)

Crucial for large tables!

- **OPTIMIZE:** Compacts small files into larger ones (1GB target).
- **Z-ORDER BY (col):** Co-locates related data (space-filling curve) to skip data during queries.

```sql
-- Run nightly or after big loads
OPTIMIZE my_table ZORDER BY (customer_id, date);
```

### 5. Vacuum (Cleanup)

Delete old files no longer needed by time travel.

```sql
-- Remove files older than 7 days (default)
VACUUM my_table;

-- Remove files older than 1 hour (Requires setting check retention)
SET spark.databricks.delta.retentionDurationCheck.enabled = false;
VACUUM my_table RETAIN 0 HOURS;
```

---

## ðŸ’§ Liquid Clustering (New!)

Replaces Hive Partitioning and Z-Order for most use cases. It flexibly reclusters data as it grows.

```sql
CREATE TABLE my_table (id INT, region STRING)
USING DELTA
CLUSTER BY (region);
```
**Advantage:** No skew issues, automatic optimization.

---

## ðŸŽ¯ Interview Q&A

**Q: What is the `_delta_log`?**
A: It is the single source of truth for the table. It records every transaction (commit) in sequence. Spark reads the log first to know which Parquet files are valid/current before reading the data.

**Q: Can you execute `VACUUM` with retention 0?**
A: Technically yes (if check disabled), but it is **dangerous**. If a reader is currently running a query on those files, the job will fail. It also prevents any time travel. Standard validation period is 7 days.

**Q: When to use Partitioning vs Z-Order vs Liquid Clustering?**
A:
- **Partitioning:** Good for huge tables (>1TB) where queries filter by low-cardinality columns (Date, Region).
- **Z-Order:** Good for high-cardinality columns (ID, timestamp) within partitions.
- **Liquid Clustering:** The modern replacement. Databricks recommends this for new tables as it handles skew and clustering dynamically without manual tuning.

---

**Next:** [Day 3: Databricks SQL](./02_Day_03.md)
