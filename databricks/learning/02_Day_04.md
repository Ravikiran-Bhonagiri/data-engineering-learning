# Day 4: Medallion Architecture

**Time:** 1-2 hours  
**Goal:** Implement the Multi-Hop (Bronze-Silver-Gold) architecture with best practices.

---

## üèÖ The Layers

### 1. Bronze (Raw)
- **Source:** Ingested from Kafka, S3, APIs.
- **State:** Raw, untouched, immutable. Append-only.
- **Format:** Delta (often tracking headers, metadata, `_ingested_timestamp`).
- **Goal:** "I can always replay my data from here."

### 2. Silver (Cleansed/Enriched)
- **Source:** Reads from Bronze.
- **Transformations:** Deduplication, schema validation, type casting, null handling.
- **Joins:** Lookup tables (e.g., enriching IDs with Names).
- **Goal:** "Single Source of Truth." Analysis-ready but normalized (3rd Normal Form adjacent).

### 3. Gold (Aggregated/Curated)
- **Source:** Reads from Silver.
- **Transformations:** Aggregations, business logic, star schemas (Facts/Dims).
- **Goal:** "Business Ready." Serving layer for Dashboards, BI, and ML.

---

## üõ† Hands-on: Coding the Layers

### Bronze Ingestion (Auto Loader)
**Auto Loader (`cloudFiles`)** efficiently detects new files in cloud storage.

```python
# Streaming Read from JSON dump
df_bronze = spark.readStream \
    .format("cloudFiles") \
    .option("cloudFiles.format", "json") \
    .option("cloudFiles.schemaLocation", "/tmp/schema/bronze") \
    .load("/mnt/landing-zone/orders/")

# Add metadata
from pyspark.sql.functions import current_timestamp
df_bronze = df_bronze.withColumn("ingestion_time", current_timestamp())

# Write to Delta table
df_bronze.writeStream \
    .format("delta") \
    .option("checkpointLocation", "/tmp/checkpoints/bronze") \
    .table("bronze_orders")
```

### Silver Transformation (Merge)

```python
# Read Bronze Stream
df_stream = spark.readStream.table("bronze_orders")

# Apply Logic (dedupe)
# NOTE: Streaming dedupe requires watermarking, but batch merge is common for Silver.

def upsert_silver(batch_df, batch_id):
    from delta.tables import DeltaTable
    silver_table = DeltaTable.forName(spark, "silver_orders")
    
    (silver_table.alias("t")
     .merge(batch_df.alias("s"), "t.order_id = s.order_id")
     .whenMatchedUpdateAll()
     .whenNotMatchedInsertAll()
     .execute())

# Execute Stream
df_stream.writeStream \
    .foreachBatch(upsert_silver) \
    .option("checkpointLocation", "/tmp/checkpoints/silver") \
    .start()
```

---

## üîÅ Replaying Data

Since you keep raw history in Bronze, if you change a business rule in Silver:
1. Delete/Truncate Silver table.
2. Reset Checkpoint for Silver stream.
3. Rerun Silver job reading from Bronze.

---

## üéØ Interview Q&A

**Q: Why use the Medallion Architecture?**
A: It provides data quality and lineage. Bronze ensures we never lose data. Silver provides a trusted, clean layer separate from specific report needs. Gold optimizes for read performance. It decouples data ingestion from business reporting.

**Q: What is Auto Loader?**
A: Auto Loader (`cloudFiles`) is a Databricks mechanism to incrementally and efficiently ingest files from cloud storage (S3/ADLS) into Delta tables. It scales better than standard `spark.readStream` file source and handles schema inference/evolution automatically.

**Q: Where do you handle duplicates?**
A: Typically in the **Silver** layer. Bronze accepts duplicates (raw). Silver performs deduplication (MERGE or `dropDuplicates`) to establish the single entry per unique key.

---

**Next:** [Day 5: Performance Tuning](./02_Day_05.md)
