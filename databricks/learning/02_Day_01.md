# Day 1: Architecture & Platform Setup

**Time:** 1-2 hours  
**Goal:** Master the Databricks workspace, clusters, and notebook environment.

---

## ðŸ— Workspace Tour

### 1. Left Navigation Bar
- **Workspace:** File browser for notebooks and code.
- **Repos:** Git integration (GitHub, Azure DevOps, GitLab).
- **Compute:** Manage clusters and warehouses.
- **SQL Editor:** Run queries and build dashboards.
- **Catalog (Unity Catalog):** Data explorer and governance.
- **Workflows:** Job scheduler.

### 2. Compute Configuration (Interview Key!)
When creating a **Compute Cluster**, you must choose a Policy and Runtime.

**Policies:**
- **Unrestricted:** Full control (Admins).
- **Personal Compute:** Single-user, cost-controlled.
- **Power User:** Shared resources.

**Access Modes (Unity Catalog):**
- **Single User:** For R/Python/Scala. One user assigned.
- **Shared:** For SQL/Python. Multiple users. Isolation enabled.
- **No Isolation:** Legacy mode (Avoid for governance).

---

## ðŸ›  Hands-on: Create a Cluster

1. Go to **Compute** -> **Create Compute**.
2. **Name:** `dev-cluster`
3. **Policy:** Unrestricted (or Personal Compute).
4. **Access Mode:** Single User (for full PySpark support).
5. **Databricks Runtime:** LTS (Long Term Support) version (e.g., 13.3 LTS).
6. **Worker Type:** `Standard_DS3_v2` (Azure) or `i3.xlarge` (AWS).
7. **Autotermination:** 30 minutes (Save costs!).
8. Click **Create Compute**.

---

## ðŸ““ Notebook Basics

Databricks notebooks support multiple languages in the same file.

```python
# Default language (Python)
print("Hello Databricks")
```

```sql
-- Magic command to switch to SQL
%sql
SELECT "Hello SQL" as greeting
```

```bash
# Magic command for shell
%sh
ls -l /dbfs
```

```markdown
# Magic command for documentation
%md
## Documentation is built-in!
```

---

## ðŸ’¾ DBFS (Databricks File System)

DBFS is a distributed file system mounted to the workspace.

- **Root:** `/` or `dbfs:/`
- **FileStore:** `/FileStore` (Persistent files)
- **Mounts:** `/mnt` (External S3/ADLS buckets)

**Best Practice:**
Do NOT store production data in DBFS root. Use mounted object storage (S3/ADLS) or external locations via Unity Catalog.

```python
# List files with dbutils (Databricks Utilities)
dbutils.fs.ls("/")

# Read a file
df = spark.read.csv("dbfs:/FileStore/data.csv")
```

---

## ðŸŽ¯ Interview Q&A

**Q: What is the difference between specific Access Modes (Single User vs Shared)?**
A: **Single User** is assigned to one identity and supports all languages (Python/R/Scala) and library installation. **Shared** is for concurrent users, prioritizing isolation and security, often limiting non-isolated code (like Scala or RDD API) to ensure governance compliance with Unity Catalog.

**Q: What happens when a cluster autoterminates?**
A: The cloud VMs are shut down to stop billing. The cluster configuration remains. When a user runs a command or a job starts, the cluster automatically restarts (if auto-start is enabled).

**Q: Explain split availability (Spot vs On-Demand).**
A: You can configure worker nodes to use Spot instances (cheaper, interruptible) and the Driver node to use On-Demand (stable). This optimizes cost for reliability.

---

**Next:** [Day 2: Delta Lake Mastery](./02_Day_02.md)
