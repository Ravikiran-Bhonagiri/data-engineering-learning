# Day 6: Delta Live Tables (DLT)

**Time:** 1-2 hours  
**Goal:** Build resilient, declarative data pipelines with quality controls.

---

## üåä What is DLT?

DLT is a **declarative** framework for building ETL pipelines. Instead of defining *how* to process data (imperative), you define *what* the data should look like.

**Key Features:**
- **Autoscaling:** Enhanced scaling for streaming.
- **Data Quality:** "Expectations" (constraints).
- **Lineage:** Automatic dependency tracking.
- **Maintenance:** Automatic optimization and vacuum.

---

## üõ† Hands-on: Building a DLT Pipeline

DLT pipelines are written in Python or SQL notebooks, then run via the **Workflows** tab (not interactive cells).

### Python Syntax (`dlt` module)

```python
import dlt
from pyspark.sql.functions import *

# 1. Bronze: Raw Ingestion with Auto Loader
@dlt.table(
    comment="Raw orders from landing zone",
    table_properties={"quality": "bronze"}
)
def bronze_orders():
    return (
        spark.readStream.format("cloudFiles")
        .option("cloudFiles.format", "json")
        .load("/mnt/raw/orders")
    )

# 2. Silver: Clean data with Expectations
@dlt.table(
    comment="Cleaned orders"
)
@dlt.expect_or_drop("valid_id", "order_id IS NOT NULL")
@dlt.expect_or_fail("positive_price", "amount > 0")
def silver_orders():
    return (
        dlt.read_stream("bronze_orders")
        .select("order_id", "date", "amount")
    )
```

### SQL Syntax

```sql
CREATE OR REFRESH STREAMING TABLE bronze_orders
AS SELECT * FROM cloud_files("/dlt-data/orders", "json")

CREATE OR REFRESH STREAMING TABLE silver_orders
(CONSTRAINT valid_date EXPECT (order_date IS NOT NULL) ON VIOLATION DROP ROW)
AS SELECT * FROM STREAM(LIVE.bronze_orders)
```

---

## üõ°Ô∏è Expectations (Data Quality)

DLT handles bad data gracefully.

1. **`expect`**: Warn (log metric) but keep data.
2. **`expect_or_drop`**: Drop the invalid row.
3. **`expect_or_fail`**: Fail the whole pipeline.

---

## üîÅ Continuous vs Triggered

- **Continuous:** Runs 24/7. Lowest latency. Higher cost.
- **Triggered:** Runs once, processes new data, shuts down. Cost-efficient (batch).

---

## üéØ Interview Q&A

**Q: What is the difference between a standard Spark Streaming Job and DLT?**
A: Standard Spark requires managing checkpoints, orchestration, and error handling manually. DLT abstracts this. It manages state, schemas, and infrastructure automatically. It also provides built-in Data Quality monitoring (Expectations) which standard Spark does not have.

**Q: Can DLT handle CDC (Change Data Capture)?**
A: Yes, using `dlt.apply_changes()` (SCD Type 1 and Type 2). It simplifies the logic of merging updates/deletes into target tables, which is complex in standard PySpark.

**Q: How do you debug DLT?**
A: DLT logs all events to a centralized **Event Log** (a Delta table). You can query this table to see data quality metrics, pipeline latency, and errors.

---

**Next:** [Day 7: Unity Catalog](./02_Day_07.md)
