# Day 10: Databricks Interview Masterclass

**Time:** 2+ hours  
**Goal:** Prepare for the technical interview with scenario-based questions.

---

## üèÜ Top 10 Technical Questions

1. **Explain the Life of a Spark Query on Databricks.**
   - Driver parses code -> Catalyst Optimizer (Logical/Physical Plans) -> DAG Scheduler (Stages) -> Task Scheduler -> Executors run Tasks.

2. **What creates the "Small File Problem" and how do you fix it?**
   - **Cause:** Streaming ingestion or frequent small inserts.
   - **Impact:** High metadata overhead, slow listings.
   - **Fix:** `OPTIMIZE` (compaction) and Auto-Optimize.

3. **Compare Merge-on-Read vs Copy-on-Write in Delta.**
   - **Copy-on-Write (Default):** Rewrites entire parquet files for updates. Slower writes, faster reads.
   - **Merge-on-Read (Deletion Vectors):** Writes soft-deletes to a small mapping file. Faster writes.

4. **How do you handle Schema Evolution in a recurring job?**
   - Use `.option("mergeSchema", "true")`. For DLT, it handles it automatically.

5. **Why use a Service Principal for jobs?**
   - Decouples the job from a human user. If the user leaves, the job doesn't fail. Better security auditing.

6. **What is Z-Ordering? Can I Z-Order on all columns?**
   - Co-locates data. No, don't Z-Order on everything. Effectiveness drops after 3-4 columns. Pick high-cardinality filter columns.

7. **How does Databricks Autoscaling differ from Standard Spark Autoscaling?**
   - Databricks "Optimized Autoscaling" is faster (scales up aggressively) and removes nodes gracefully without losing shuffle data (if configured correctly).

8. **Explain the cost difference between interactive clusters and job clusters.**
   - Job clusters are significantly cheaper (~50% less DBU cost) because they don't carry the overhead of the interactive web environment/notebook support.

9. **What is a "Broadcast Join" and when does it fail?**
   - Copying the small table to all nodes. Fails with OOM if the "small" table is actually big (>10GB/Driver Limit).

10. **How do you secure PII statistics in Databricks?**
    - Unity Catalog Dynamic Views (Column masking) or Column-level permissions.

---

## üé≠ Scenario-Based Questions

### Scenario 1: The Slow Stream
**Problem:** "My streaming job processes data fine, but every 6 hours it hangs for 20 minutes."
**Diagnosis:**
- Check for **Checkpoint iterations**.
- Is `OPTIMIZE` running on the source table?
- **Likely Cause:** Compaction or a huge batch of data arriving at that time.
- **Fix:** If source is Delta, use `maxBytesPerTrigger` to smooth out valid spikes.

### Scenario 2: The Expensive Cluster
**Problem:** "Our dev team is spending $10k/month on interactive clusters."
**Solution:**
- Implement **Cluster Policies**: Force autotermination to 30 mins.
- Use **Personal Compute** policies to limit instance sizes.
- Audit usage: Are they running production jobs on interactive clusters? Move to **Job Clusters**.

### Scenario 3: Data Quality Disaster
**Problem:** "Bad data got into Gold tables. Dashboards are wrong."
**Solution:**
- Time Travel! `RESTORE TABLE Gold TO VERSION AS OF <yesterday>`.
- Fix the bug in Silver/Bronze.
- Rerun pipeline.
- **Prevention:** Implement DLT Expectations to drop/alert bad data *before* it propagates.

---

## üèÅ Final Prep

1. **Review:** Read all `03_QUICK_Reference` commands.
2. **Practice:** Write a MERGE statement from scratch on paper.
3. **Architecture:** Be able to draw the Lakehouse diagram (Control vs Data plane) on a whiteboard.

**Good Luck! You are ready.** üöÄ
